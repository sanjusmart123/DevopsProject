controlplane $ vi pod.yml
controlplane $ kubectl create -f pod.yml
pod/mysql created
controlplane $ kubectl get pod
NAME    READY   STATUS              RESTARTS   AGE
mysql   0/1     ContainerCreating   0          8s
controlplane $ kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
mysql   1/1     Running   0          13s
controlplane $ cat pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  containers:
   - name: mysql-container
     image: mysql:5
     env:
      - name: MYSQL_ROOT_PASSWORD
        value: sai637
      - name: MYSQL_DATABASE
        value: fleetman
controlplane $ vi service.yml
controlplane $ kubectl create -f service.yml
service/database created
controlplane $ kubectl get all
NAME        READY   STATUS    RESTARTS   AGE
pod/mysql   1/1     Running   0          8m38s

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
service/database     ClusterIP   10.97.71.68   <none>        3306/TCP   33s
service/kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP    19d
controlplane $ kubectl exec -it mysql
error: you must specify at least one command for the container
controlplane $ kubectl exec -it -c  mysql
error: pod, type/name or --filename must be specified
controlplane $ kubectl exec -it -c pod/mysql
error: pod, type/name or --filename must be specified
controlplane $ kubectl exec -it pod  pod/mysql
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Error from server (NotFound): pods "pod" not found
controlplane $ kubectl exec -it pods  pod/mysql
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Error from server (NotFound): pods "pods" not found
controlplane $ cat service.yml
apiVersion: v1
kind: Service
metadata:
  name: database
spec: 
  selector:
    app: mysql
  ports:
  - port: 3306
  type: ClusterIP
controlplane $ ls
filesystem  pod.yml  service.yml  snap
controlplane $ kubectl exec -it mysql -- /bin/bash
bash-4.2# ls
bin   dev                         entrypoint.sh  home  lib64  mnt  proc  run   srv  tmp  var
boot  docker-entrypoint-initdb.d  etc            lib   media  opt  root  sbin  sys  usr
bash-4.2# exit

First, you need to create a Service that exposes the MySQL Pod to the network. 
Then, you can use a Deployment,Pod or a StatefulSet to create multiple replicas of the MySQL Pod. 
To connect to the MySQL database from within the Pod, you can use the Pod's IP address and the MySQL port. 
Alternatively, you can use a Persistent Volume Claim (PVC) to persist data even if the Pod is restarted or deleted. 
Additionally, you can use a Kubernetes Ingress to expose the MySQL Service to the outside world.


Rolling Updates and Rollbacks in Deployments:

A Rolling Update is a deployment strategy that allows you to update the application in a way that replaces old versions of the application with new versions gradually,
without causing downtime. The key idea is to update pods one at a time or in small batches, ensuring that the application remains available throughout the update process

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1          # Allow one additional pod to be created during the update
      maxUnavailable: 25%  # Allow 25% of the pods to be unavailable during the update
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app-container
          image: my-app:v2
          ports:
            - containerPort: 8080

maxSurge: 1: Allows one extra pod to be created during the update, which helps minimize downtime.
maxUnavailable: 25%: Allows up to 25% of the pods to be unavailable during the update process.

Kubernetes starts by creating the new version of the application and scaling up the new pods (subject to the maxSurge setting).
The old version of the application continues running during the update process, ensuring that the number of available pods is maintained.
Once the new pod(s) are healthy (according to readiness/liveness checks), Kubernetes scales down the old version (subject to the maxUnavailable setting).
The process continues until all the old pods are replaced by the new version.
This process ensures that the application remains available and responsive during the update.

####################################################################################################

A Rollback is a process of reverting to a previous version of a deployment if something goes wrong during the rolling update or if the new version causes issues in production.

How Rollbacks Work:
Automatic Rollback: Kubernetes supports automatic rollbacks if the health checks (readiness/liveness probes) fail or if the deployment doesn't proceed as expected.
It will revert the deployment to the previous stable version.
Manual Rollback: You can also manually trigger a rollback to a specific version using the kubectl rollout undo command.

Check the Deployment History: Kubernetes keeps a history of previous deployments, 
and you can use the kubectl rollout history command to view the history of a deployment and find the previous revision.

kubectl rollout history deployment/my-app

Rollback to Previous Version: Use the kubectl rollout undo command to roll back to the previous revision.
kubectl rollout undo deployment/my-app

If you want to roll back to a specific revision:
kubectl rollout undo deployment/my-app --to-revision=2

##########################################################################################################

Sleep command:
Pod Lifecycle Testing: You can use sleep to test Kubernetes Pod scheduling, rolling updates, and termination behaviors. 
A long sleep period helps test how the system handles Pods that run for an extended time or are stuck in a state (for example, simulating a "hanging" container).
Kubernetes Readiness and Liveness Probes: During testing, sleep can be useful for verifying the effectiveness of Kubernetes health checks. 
You can experiment with liveness and readiness probes by configuring them to check the container's state while it sleeps for a long period.

Background or Asynchronous Jobs: In many production environments, certain jobs run in the background, such as batch processing, cron jobs, or other asynchronous tasks.
The sleep command can simulate this scenario, where a container stays idle but doesn't exit until it is manually terminated or the sleep duration ends.
Workload Simulation: If you want to simulate background workloads that don’t have a predefined end, sleep can be a simple placeholder while you work on building or testing a more complex service.

apiVersion: v1                # API version used to define the object.
kind: Pod                      # The kind of resource; in this case, a Pod.
metadata:
  name: ubuntu-sleeper-3       # Name of the Pod, you can reference it within the Kubernetes cluster.
spec:
  containers:                  # Specifies the container configuration within the Pod.
  - name: ubuntu               # The name of the container inside the Pod.
    image: ubuntu              # The Docker image to be used for the container, here it's the official Ubuntu image.
    command:                   # The command to run inside the container (overrides the default command in the image).
      - "sleep"                # The command that runs when the container starts.
      - "2000"                 # The argument passed to the sleep command; the container will sleep for 2000 seconds.

###############################################################################################################################

Configure Environment Variables In Applications:

Environment variables (often abbreviated as env) are crucial in Kubernetes, Docker, and many application deployments. They are used to configure application behavior, store sensitive information,
Configuration Management:

Externalize Configuration: Store configuration settings such as database URLs, API keys, or application parameters in environment variables to keep them separate from the application code. 
This makes it easier to modify configurations without changing the codebase.

env:
  - name: DATABASE_URL
    value: "postgres://user:password@host/dbname

Environment-Specific Configuration: 
Use environment variables to distinguish between different environments (development, testing, production). For instance, use APP_ENV to define the environment type and load different settings accordingly.

env:
  - name: APP_ENV
    value: "production"

Security & Sensitive Information:

Store Secrets: Rather than hard-coding sensitive information (such as passwords, API tokens, or private keys) directly in the application or source code, you can store them as environment variables. Kubernetes has a built-in resource type called Secrets for this purpose, which can be mapped to environment variables.
Example: Using Kubernetes secrets to inject sensitive data.

env:
  - name: DB_PASSWORD
    valueFrom:
      secretKeyRef:
        name: mysecret
        key: password

Environment variables are a flexible and powerful way to control the configuration, behavior, and security of your applications in Kubernetes. 
They allow you to decouple the configuration from your application code, support different environments, manage secrets securely, and provide fine-grained control over application behavior.

##########################################################################################################################

ConfigMaps in Kubernetes are used to store configuration data as key-value pairs. 
Storing configuration data such as environment variables, config files, command-line arguments


kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2

kubectl create configmap my-config --from-file=path/to/configfile --> created from a file


##################################
Inject configmap in pod

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod
kubectl create -f pod-definition.yaml


apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
 containers:
 - name: simple-webapp-color
   image: simple-webapp-color
   ports:
   - containerPort: 8080
   envFrom:
   - configMapRef:
       name: app-config

Create a config map definition file and run the 'kubectl create` command to deploy it.
kubectl create -f config-map.yaml

 kubectl get configmaps (or)
 kubectl get cm

kubectl describe configmaps


#####################################

Secretes:

A Secret is used to store sensitive information, such as passwords, tokens, or keys. While data in a Secret is also stored as key-value pairs, it is base64-encoded, not encrypted by default (though encryption at rest can be enabled).

Use case: Storing sensitive data like database credentials, API keys, SSL certificates, etc.
Format: Base64-encoded data, though you should treat it as confidential information.

kubectl create secret generic my-secret --from-literal=username=admin --from-literal=password='s3cr3t'

kubectl create secret generic my-secret --from-file=path/to/secretfile

Yaml file:

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  username: YWRtaW4=   # base64 encoded value of 'admin'
  password: c2VjcmV0   # base64 encoded value of 'secret'


echo -n 'admin' | base64
echo -n 'secrete' | base64   --->to convert encoded

echo -n 'serty45' | base64 --decode


Accessing secrete from env:

env:
- name: SECRET_USERNAME
  valueFrom:
    secretKeyRef:
      name: my-secret
      key: username

from Volumes:

volumes:
  - name: secret-volume
    secret:
      secretName: my-secret

###################################################################################################################
Multi-container-Pods:

In Kubernetes, multi-container pods are used to deploy multiple containers within the same pod. 
Each container in the pod shares the same network namespace (i.e., they can communicate with each other via localhost), 
and they are co-located, sharing resources like volumes, which makes multi-container pods useful for specific use cases where containers need to work together. 

apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - ContainerPort: 8080
  - name: log-agent
    image: log-agent
##################################

Sidecar Pattern
Description: The sidecar pattern involves running a helper container alongside the main application container. 
The sidecar container provides supplementary functionality, such as logging, monitoring, or proxying, which supports the main application.
Use Case: A typical use case is running a logging agent (like Fluentd or Filebeat) alongside a web application container.
The logging container collects logs and forwards them to a logging service or stores them in a centralized system.

Example: A pod with a main web application and a sidecar logging container:

yaml
Copy code
apiVersion: v1
kind: Pod
metadata:
  name: app-with-logging
spec:
  containers:
  - name: app-container
    image: my-app
  - name: logging-agent
    image: fluentd
    volumeMounts:
    - name: logs
      mountPath: /var/log
  volumes:
  - name: logs
    emptyDir: {}

####################################################################################################
Init Containers:

Init Containers in Kubernetes are specialized containers that run before the main application containers in a pod.
They allow you to perform initialization tasks that must complete before the main containers start running.
Init containers run sequentially, and each init container must finish successfully before the next one starts. 
If any init container fails, Kubernetes will restart the pod until the initialization succeeds.

Use Case: For example, you might need to set up a configuration file or initialize a database schema before the main application container can start handling traffic.
Benefit: This ensures that the main container only starts once all the necessary initialization tasks have been completed successfully.

apiVersion: v1
kind: Pod
metadata:
  name: app-with-db-init
spec:
  initContainers:
  - name: db-init
    image: db-init-image
    command: ["sh", "-c", "echo 'CREATE DATABASE mydb;' | mysql -h db_host"]
  containers:
  - name: main-app
    image: my-app

###########################################################################################################
We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable.
kubectl drain node01 --ignore-daemonsets
This command:
Evicts all pods from the node (except DaemonSet pods).
Ensures that no new pods are scheduled on the node

kubectl cordon node01 --->no new pods assigned to node01 until it set to uncordon state

kubectl uncordon node01  --->The maintenance tasks have been completed. Configure the node node01 to be schedulable again

Why are the pods placed on the controlplane node? ----> Controlplane does not have taints

#################################################################################################

Upgrading a cluster involves 2 major steps
There are different strategies that are available to upgrade the worker nodes
One is to upgrade all at once. But then your pods will be down and users will not be able to access the applications.

Second one is to upgrade one node at a time.

Third one would be to add new nodes to the cluster


kubeadm - Upgrade master node
kubeadm has an upgrade command that helps in upgrading clusters.

kubeadm upgrade plan 

Upgrade kubeadm from v1.11 to v1.12

$ apt-get upgrade -y kubeadm=1.12.0-00
Upgrade the cluster

$ kubeadm upgrade apply v1.12.0
If you run the 'kubectl get nodes' command, you will see the older version. This is because in the output of the command it is showing the versions of kubelets on each of these nodes registered with the API Server and not the version of API Server itself

$ kubectl get nodes

Upgrade 'kubelet' on the master node

$ apt-get upgrade kubelet=1.12.0-00
Restart the kubelet

$ systemctl restart kubelet
Run 'kubectl get nodes' to verify

$ kubectl get nodes

kubeadm - Upgrade worker nodes
From master node, run 'kubectl drain' command to move the workloads to other nodes

$ kubectl drain node-1
Upgrade kubeadm and kubelet packages

$ apt-get upgrade -y kubeadm=1.12.0-00
$ apt-get upgrade -y kubelet=1.12.0-00
Update the node configuration for the new kubelet version

$ kubeadm upgrade node config --kubelet-version v1.12.0
Restart the kubelet service

$ systemctl restart kubelet
Mark the node back to schedulable
$ kubectl uncordon node-1

Upgrade all worker nodes in the same way


#################################################################################################

steps to Update an EKS Cluster
1. Check the Current EKS Cluster Version
Run the following command to check the current Kubernetes version of your cluster:

aws eks describe-cluster --name <cluster-name> --query "cluster.version" --output text

2. Check Available Updates
List available Kubernetes versions for your region:

aws eks describe-addon-versions --query "addons[].kubernetesVersions" --output text

3. Update the Control Plane
To update the EKS control plane to the latest version:

aws eks update-cluster-version --name <cluster-name> --kubernetes-version <target-version>

4. Update Node Groups
a. Update Managed Node Groups
For managed node groups, update the node group's Kubernetes version:

aws eks update-nodegroup-version --cluster-name <cluster-name> --nodegroup-name <node-group-name> --kubernetes-version <target-version>

kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

5. Update Add-ons
After updating the cluster and nodes, update the EKS add-ons (e.g., CoreDNS, kube-proxy).

Check Add-ons Versions
bash
Copy code
aws eks describe-cluster --name <cluster-name> --query "cluster.addons" --output json
Update Add-ons
For example, to update CoreDNS:

aws eks update-addon --cluster-name <cluster-name> --addon-name coredns --addon-version <target-version>
Monitor the add-on update:

aws eks describe-addon --cluster-name <cluster-name> --addon-name coredns --query "addon.status" --output text

6. Verify the Update
After all updates:
Confirm the Kubernetes version:
kubectl version --short
Check node readiness:

kubectl get nodes
Validate add-ons:

kubectl get pods -n kube-system

#####################################################################################################################

Backup and Restore Methods:

2. Application-Level Backup and Restore
Backing Up Resources
Use kubectl to export YAML definitions of all resources:


kubectl get all --all-namespaces -o yaml > cluster-backup.yaml
Restoring Resources
Apply the YAML file to restore resources:

kubectl apply -f cluster-backup.yaml
Note: This does not backup Persistent Volumes (PVs) or data stored within pods.

You can take a snapshot of the etcd database by using etcdctl utility snapshot save command.

ETCDCTL_API=3 etcdctl snapshot save snapshot.db
ETCDCTL_API=3 etcdctl snapshot status snapshot.db

Restore - ETCD
To restore etcd from the backup at later in time. First stop kube-apiserver service

 service kube-apiserver stop
Run the etcdctl snapshot restore command

Update the etcd service

Reload system configs

systemctl daemon-reload
Restart etcd

service etcd restart

Start the kube-apiserver

 service kube-apiserver start

With all etcdctl commands specify the cert,key,cacert and endpoint for authentication.
ETCDCTL_API=3 etcdctl \
  snapshot save /tmp/snapshot.db \
  --endpoints=https://[127.0.0.1]:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/etcd-server.crt \
  --key=/etc/kubernetes/pki/etcd/etcd-server.key
Other way:
Using Tools for Cluster Backup
Several tools automate cluster and application backups:

Velero
#############################################################################################################







In Kubernetes, a Cluster Role is a set of permissions that define what actions a user or service account can perform within a cluster.
It's essentially a collection of verbs (actions) that can be applied to a set of resource types, such as pods, services, or persistent volumes. 
Cluster Roles are used to manage access control and ensure that resources are only accessed and modified by authorized entities. 
They are often associated with Service Accounts, which are used to authenticate and authorize processes running within a pod.

Cluster Role:
#################
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: read-all-pods-and-nodes
rules:
- apiGroups: [""]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]


######################
Cluster Role Binding:
To assign this ClusterRole to a specific user, group, or service account, you use a ClusterRoleBinding.
This binding allows you to specify who can use the defined permissions in the ClusterRole.
######################
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-all-pods-and-nodes-binding
subjects:
- kind: User
  name: "jane"  # The user who will be granted access
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: read-all-pods-and-nodes
  apiGroup: rbac.authorization.k8s.io

###############################
ConfigMaps in Kubernetes are used to store configuration data as key-value pairs. 
Storing configuration data such as environment variables, config files, command-line arguments


kubectl create configmap my-config --from-literal=key1=value1 --from-literal=key2=value2

kubectl create configmap my-config --from-file=path/to/configfile --> created from a file

from yml file:

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color

Accessing Configmap:

envFrom:
- configMapRef:
    name: my-config

#####################################

Secretes:

A Secret is used to store sensitive information, such as passwords, tokens, or keys. While data in a Secret is also stored as key-value pairs, it is base64-encoded, not encrypted by default (though encryption at rest can be enabled).

Use case: Storing sensitive data like database credentials, API keys, SSL certificates, etc.
Format: Base64-encoded data, though you should treat it as confidential information.

kubectl create secret generic my-secret --from-literal=username=admin --from-literal=password='s3cr3t'

kubectl create secret generic my-secret --from-file=path/to/secretfile

Yaml file:

apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  username: YWRtaW4=   # base64 encoded value of 'admin'
  password: c2VjcmV0   # base64 encoded value of 'secret'


echo -n 'admin' | base64
echo -n 'secrete' | base64   --->to convert encoded

echo -n 'serty45' | base64 --decode


Accessing secrete from env:

env:
- name: SECRET_USERNAME
  valueFrom:
    secretKeyRef:
      name: my-secret
      key: username

from Volumes:

volumes:
  - name: secret-volume
    secret:
      secretName: my-secret


what is pod in kubernetes:

In Kubernetes, a Pod is the basic execution unit and deployable object that represents a single instance of a running application or service. 
A Pod is made up of one or more Containers, which are run alongside each other in the same process space.
Pods are ephemeral, meaning they can be removed and recreated as needed. 
Each Pod has its own IP address and port numbers, allowing multiple Pods to run on the same node without conflicts.
Pods are the smallest unit of scalability and fault tolerance in Kubernetes, making them a fundamental concept in containerized orchestration.
if a pod (or the node it executes on) fails, Kubernetes can automatically create a new replica of that pod to continue operations.


To create pod
kubectl run nginx --image=nginx
Get status of pod
kubectl get pods
all info 
kubectl get all
Syntax of yaml file
kubectl explain pods
To create individual pod by using yaml file
vi test.yml

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: nginx
    ports:
    - containerPort: 80
then
kubectl create -f test.yml 
to delete pod
kubectl delete pod my-pod
all info,syntax of pods
kubectl explain pods --recursive





Create Deployment pod:
vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: nginx
        ports:
        - containerPort: 80
kubectl create -f deploy.yml
in Deployemt we can change replicas,we can give 5
then..apply changes
kubectl apply -f deploy.yml
we can change image
kubectl set image deployment/my-deployment(podName) my-container(containerName)=nginx:latest
here changed image version ,we can also rollback to previous version by
kubectl rollout undo deployment/my-deployment(podName)
we can chnages here
kubectl get replicaset
when we give wrong image name,we can see info why our image fail
kubectl describe deployment.apps/my-deployment


in deployment,we delete one pod i will create another pod
we can delete pod permanently 
kubectl delete deployment/pod_name

kubectl get po --show-labels ----> display labels 
kubectl exec -it mysql -- /bin/bash ----> to enter into specific container or pod
kubectl delete cluster ${NAME} --yes ---> to delete a cluster




selector:
 it is used for grouping,we are having multiple deployment files and service file is supposed to be same 
so how service file is identify the with the network access provide,
then how would service know that on which deployment or pod we are supposed to provide network access,
thats the reason selector is used it will match the labels inside the selector

Template:

Actually it is a blue print to create a pod.

Specifications:

spec contains details about the container

containerPort:

containerport that will be opened on the container that will be created inside he pod 

nodePort:
it will give accesss for over the outside the internet,
for im going to deploy this deployment and service files from this nginx server is deployed,
to access that server we will use this nodeport

nodeport will be range between 30000 to 32768


A DaemonSet is a type of Kubernetes object that ensures a copy of a pod is running on every node in a cluster.
It is used to run a service on every node, such as a monitoring agent, a logging collector, or a network daemon.
DaemonSets are useful for running long-running tasks that require access to the node's resources, like disk storage or network interfaces.
They can also be used to maintain a consistent state across all nodes in the cluster. DaemonSets are created by submitting a DaemonSet manifest to the Kubernetes API.

Runs Pods on Every Node:

A DaemonSet automatically schedules one pod on every eligible node in the cluster.
Dynamic Updates:

If new nodes are added to the cluster, the DaemonSet automatically schedules the pod on those nodes.
Similarly, if a node is removed, its associated pod is deleted.
Selectors for Node Targeting:

DaemonSets can target specific nodes using node selectors, node affinity, or taints and tolerations.
Immutable Updates:

Updating a DaemonSet (e.g., changing its image) requires rolling out changes to all nodes.

Monitoring and Logging Agents:

Tools like Prometheus Node Exporter, Fluentd, or Datadog that collect node-specific metrics or logs.
Networking and Security:

Services like CNI plugins, VPN agents, or firewall configurations.
Cluster Administration:

System-level tasks like garbage collection, resource monitoring, or node-level configuration.

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
controlplane $ vi deploy.yml
controlplane $ kubectl create -f deploy.yml
deployment.apps/my-deployment created
controlplane $ kubectl get pods
NAME                             READY   STATUS              RESTARTS   AGE
my-deployment-56fb9fb7fc-2xc5l   0/1     ContainerCreating   0          10s
my-deployment-56fb9fb7fc-f5bg6   0/1     ContainerCreating   0          10s
my-deployment-56fb9fb7fc-g8wdr   0/1     ContainerCreating   0          10s
controlplane $ kubectl delete pod my-deployment-56fb9fb7fc-g8wdr
pod "my-deployment-56fb9fb7fc-g8wdr" deleted
controlplane $ kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
my-deployment-56fb9fb7fc-2xc5l   1/1     Running   0          50s
my-deployment-56fb9fb7fc-f5bg6   1/1     Running   0          50s
my-deployment-56fb9fb7fc-stcnt   1/1     Running   0          12s
controlplane $ kubectl delete deployment/my-deployment-56fb9fb7fc-stcnt
Error from server (NotFound): deployments.apps "my-deployment-56fb9fb7fc-stcnt" not found
controlplane $ kubectl delete deployment.apps/my-deployment-56fb9fb7fc-stcnt
Error from server (NotFound): deployments.apps "my-deployment-56fb9fb7fc-stcnt" not found
controlplane $ kubectl get deployment                                       
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
my-deployment   3/3     3            3           8m38s
controlplane $ kubectl set image deployment/my-deployment nginx=nginx:latest
error: unable to find container named "nginx"
controlplane $ kubectl set image deployment/my-deployment my-container=nginx:latest
deployment.apps/my-deployment image updated
controlplane $ kubectl get all
NAME                                 READY   STATUS    RESTARTS   AGE
pod/my-deployment-6d45fc889b-6lkrq   1/1     Running   0          9s
pod/my-deployment-6d45fc889b-b55td   1/1     Running   0          11s
pod/my-deployment-6d45fc889b-sf6r2   1/1     Running   0          7s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   14d

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-deployment   3/3     3            3           14m

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/my-deployment-56fb9fb7fc   0         0         0       14m
replicaset.apps/my-deployment-6d45fc889b   3         3         3       11s
controlplane $ kubectl rollout undo deployment/my-deployment
deployment.apps/my-deployment rolled back
controlplane $ kubectl get all
NAME                                 READY   STATUS    RESTARTS   AGE
pod/my-deployment-56fb9fb7fc-fv9d9   1/1     Running   0          8s
pod/my-deployment-56fb9fb7fc-nrx54   1/1     Running   0          6s
pod/my-deployment-56fb9fb7fc-qhfvk   1/1     Running   0          10s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   14d

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-deployment   3/3     3            3           18m

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/my-deployment-56fb9fb7fc   3         3         3       18m
replicaset.apps/my-deployment-6d45fc889b   0         0         0       4m7s
controlplane $ kubectl get replicaset
NAME                       DESIRED   CURRENT   READY   AGE
my-deployment-56fb9fb7fc   3         3         3       20m
my-deployment-6d45fc889b   0         0         0       6m
controlplane $ vi deploy.yml
controlplane $ kubectl get all
NAME                                 READY   STATUS    RESTARTS   AGE
pod/my-deployment-56fb9fb7fc-fv9d9   1/1     Running   0          15m
pod/my-deployment-56fb9fb7fc-nrx54   1/1     Running   0          15m
pod/my-deployment-56fb9fb7fc-qhfvk   1/1     Running   0          15m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   14d

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-deployment   3/3     3            3           33m

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/my-deployment-56fb9fb7fc   3         3         3       33m
replicaset.apps/my-deployment-6d45fc889b   0         0         0       19m
controlplane $ kubectl apply -f deploy.yml
Warning: resource deployments/my-deployment is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
deployment.apps/my-deployment configured
controlplane $ kubectl get all
NAME                                 READY   STATUS              RESTARTS   AGE
pod/my-deployment-56fb9fb7fc-fv9d9   1/1     Running             0          16m
pod/my-deployment-56fb9fb7fc-nrx54   1/1     Running             0          15m
pod/my-deployment-56fb9fb7fc-qhfvk   1/1     Running             0          16m
pod/my-deployment-fb769fbf-5jk42     0/1     ContainerCreating   0          4s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   14d

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-deployment   3/3     1            3           34m

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/my-deployment-56fb9fb7fc   3         3         3       34m
replicaset.apps/my-deployment-6d45fc889b   0         0         0       20m
replicaset.apps/my-deployment-fb769fbf     1         1         0       4s
controlplane $ kubectl describe pod/my-deployment-fb769fbf-5jk42
Name:             my-deployment-fb769fbf-5jk42
Namespace:        default
Priority:         0
Service Account:  default
Node:             controlplane/172.30.1.2
Start Time:       Thu, 18 Jul 2024 15:18:38 +0000
Labels:           app=my-app
                  pod-template-hash=fb769fbf
Annotations:      cni.projectcalico.org/containerID: f0f245b611b27ff1237f370b198e395981d19e7e88825aa19d73a136a26ffe5a
                  cni.projectcalico.org/podIP: 192.168.0.16/32
                  cni.projectcalico.org/podIPs: 192.168.0.16/32
Status:           Pending
IP:               192.168.0.16
IPs:
  IP:           192.168.0.16
Controlled By:  ReplicaSet/my-deployment-fb769fbf
Containers:
  my-container:
    Container ID:   
    Image:          nginx/v1
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ErrImagePull
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5ts84 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-api-access-5ts84:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  57s                default-scheduler  Successfully assigned default/my-deployment-fb769fbf-5jk42 to controlplane
  Normal   Pulling    33s (x2 over 56s)  kubelet            Pulling image "nginx/v1"
  Warning  Failed     25s (x2 over 47s)  kubelet            Failed to pull image "nginx/v1": failed to pull and unpack image "docker.io/nginx/v1:latest": failed to resolve reference "docker.io/nginx/v1:latest": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
  Warning  Failed     25s (x2 over 47s)  kubelet            Error: ErrImagePull
  Normal   BackOff    12s (x2 over 47s)  kubelet            Back-off pulling image "nginx/v1"
  Warning  Failed     12s (x2 over 47s)  kubelet            Error: ImagePullBackOff
controlplane $ vi deploy.yml
controlplane $ kubectl apply -f deploy.yml
deployment.apps/my-deployment configured
controlplane $ kubectl get all
NAME                                 READY   STATUS    RESTARTS   AGE
pod/my-deployment-56fb9fb7fc-fv9d9   1/1     Running   0          20m
pod/my-deployment-56fb9fb7fc-nrx54   1/1     Running   0          20m
pod/my-deployment-56fb9fb7fc-qhfvk   1/1     Running   0          20m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   14d

NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-deployment   3/3     3            3           38m

NAME                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/my-deployment-56fb9fb7fc   3         3         3       38m
replicaset.apps/my-deployment-6d45fc889b   0         0         0       24m
replicaset.apps/my-deployment-fb769fbf     0         0         0       4m26s
controlplane $ 



kubectl get pods webapp-daemonset-79m8x -o jsonpath='{.spec.containers[*].name}' ---->To get a container name inside a pod


vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: nginx
        ports:
        - containerPort: 80
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp
  
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp-container
        image: richardchesterwood/k8s-fleetman-webapp-angular:release1
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: production-microservice

########
service:
#########
apiVersion: v1
kind: Service
metadata:
  name: fleetman-webapp
spec:
  selector:
    app: webapp
  ports:
    - name: http
      port: 80
      nodePort: 30080
  type: NodePort 

                                                    
ClusterIP assigns internal IP addresses for intra-cluster communication, NodePort exposes applications 
to external clients via specific ports on worker nodes, and LoadBalancer provides publicly accessible IP addresses for external traffic distribution.


In Kubernetes, NodePort and LoadBalancer are two methods to access applications deployed on a cluster from outside. 
NodePort exposes a service on a specific port on each node in the cluster, making it accessible from outside. 
LoadBalancer, on the other hand, uses an external cloud provider's load balancer to distribute traffic across nodes. 
NodePort is simpler to set up but provides less scalability and security, while LoadBalancer offers better scalability and security at the cost of more complex setup. 
Choose NodePort for small-scale applications and LoadBalancer for large-scale, high-traffic deployments.

ClusterIP is ideal for internal communication within the cluster, NodePort allows external access without a load balancer, 
and LoadBalancer is suitable for publicly accessible applications.

Docker:

Docker is a popular open-source containerization platform that enables the development, deployment, and management of applications in containers.
Containers provide a lightweight and portable way to package an application with its dependencies, allowing developers to deploy applications consistently across different environments. 
Docker provides a runtime environment for containers, making it easy to create, run, and manage multiple isolated containers on a single host OS.
This approach provides improved resource utilization, faster deployment, and enhanced scalability. 
Docker has become a widely-used tool in modern software development, DevOps, and cloud computing.

Docker is a software platform that allows you to build, test, and deploy applications quickly

Containers package an application with all its dependencies (like libraries, binaries, and configuration files), allowing it to run consistently across different environments

Docker is a containerization platform that allows developers to package, ship, and run applications in containers. 
Containers are lightweight and portable, ensuring that applications run consistently across different environments.
Docker provides a flexible way to manage dependencies, reduce conflicts, and increase collaboration among developers.
Key features include image creation, container runtime, and orchestration through tools like Docker Compose and Kubernetes.
Docker supports a wide range of languages, including Java, Python, and Node.js, making it a popular choice for modern application development and deployment. 
Its versatility has made it a standard tool for cloud-native applications.


what is docker file:

A Dockerfile is a text file that contains a series of instructions, or commands, for building a Docker image.
This file serves as a blueprint for creating a containerized application. 
The Dockerfile specifies the base image, sets environment variables, copies files and directories, and configures the application, among other tasks. 
Once a Dockerfile is created, you can build an image by running the command "docker build" followed by the path to the Dockerfile.
The resulting image can then be run as a container using the "docker run" command. 
This allows for easy deployment and management of applications in containers.
Docker version
docker search centos  -   it will search images on centos
docker apt-get update -- it will update file
sudo docker -t build hello-world --- it will create image
sudo docker -t run hello-world  ---- it will create a container
docker ps ---> show u running containers
docker ps -a ---> show u updates of all containers
docker images ---> display images 
docker run -it centos /bin/bash -->create and servor changes to centos
interactive teletype  --> it
ls -lrt   --->file on that server
exit ---> back
docker info ---> docker info
docker pull wordpress --> it will download image
docker rmi wordpress ---> it will remove image
docker rm wordpress ---> it will remove container
docker run --help ---> all info about docker
docker run -dit --name centos --hostname=centos centsos /bin/bash
docker attach centos --> change server
top --->it will show u container info
docker run centos /usr/bin/free -m ---> display free memory
docker stats --->display cpu%,limit,memory
docker start centos
docker cp centos:/etc/passwd  /root/centos.passwd  --> copy files from docker to local machine
docker rmi centos -f ---> it will remove forecefully
docker save -->image save like .tar file
docker load -->.tar file cchange to image
docker commit --->then we can push to registry
docker pause,unpause,start,stop,kill 
docker build ---> to build docker file
sudo su - ---->convert to root server
docker exec -it centos /bin/bash --->This command will give you an interactive bash shell inside the my-centos container.
You can now run commands inside this container as if you were logged into a CentOS machine.
docker inspect dockerID -->display ip address
docker exec -it <container_id or name> <command> ---Execute a command inside a running container







Create Docker file commands:

Create docker file Commands:

1. FROM --> to pull base image

2. RUN --> To execute lunux or bash Commands

3. CMD --->it provide default Commands for an executing Container

4. ENTRY POINT → Same as CMD, difference is replace Commands But Entrypoint ok Can in executing time in CMD. We Cant override

5. WORK DIR -->To Sets the working directory.

6. COPY -->To Copy a directory from our to docker Container. Local machine,

7. ADD -->To Copy files and folders from local machine! to docker,ADD is also used to download data from th einternet

8. EXPOSE -->In Containers, so many applications runs, expose displays the app port numbers

9. ENV -->to set environment Variables.


Use CMD:

When you want a default command that users can easily override.
For example: CMD ["nginx", "-g", "daemon off;"].
Use ENTRYPOINT:

When the container must always run a specific program regardless of user input.
For example: ENTRYPOINT ["nginx"].
Use both ENTRYPOINT and CMD:

For flexibility, where the container runs a default executable (ENTRYPOINT) but allows default or user-specified arguments (CMD).





Create a dockerfile...

install tomcat on Centos

* pull Centos from dockerhub Base image-->

FROM

* install Java -->RUN

* Create (opt/tomcat directory →RUN

* change work directory to → WORK DIR

* Download tomcat packages →ADD/RUN

* extract tar.gz file →RUN

* Rename to tomcat directory →RUN

* Tell to docker that it runs on port 8080 → EXPOSE

* start tomcat Services →CMD.
Docker Compose is a powerful tool for defining and running multi-container Docker applications. 
It allows you to define the services, networks, and volumes required for your application using a YAML configuration file.
With Docker Compose, you can easily start, stop, and manage your containers, as well as link containers together to create a fully functional application.
It simplifies the development and deployment process by providing a more intuitive and efficient way to manage complex Docker setups.
# Use an official Python runtime as the base image
FROM python:3.9-slim

# Set the working directory in the container
WORKDIR /app

# install required packages for system
RUN apt-get update \
    && apt-get upgrade -y \
    && apt-get install -y gcc default-libmysqlclient-dev pkg-config \
    && rm -rf /var/lib/apt/lists/*

# Copy the requirements file into the container
COPY requirements.txt .

# Install app dependencies
RUN pip install mysqlclient
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code
COPY . .

# Specify the command to run your application
CMD ["python", "app.py"]


###########################################################

# Use an official CentOS as a parent image
FROM centos:7

# Set the working directory in the container
WORKDIR /app

# Install Python and any other dependencies
RUN yum update -y && \
    yum install -y python3 python3-pip && \
    yum clean all

# Copy the current directory contents into the container at /app
COPY . /app

# Install any needed Python packages specified in requirements.txt
RUN pip3 install --no-cache-dir -r requirements.txt

# Make port 80 available to the world outside this container
EXPOSE 80

# Run the application
CMD ["python3", "my_script.py"]


Explanation:

FROM centos:7: Specifies the base image to use.

WORKDIR /app: Sets /app as the working directory inside the container.

RUN yum update -y && yum install -y python3 python3-pip: Updates the package list and installs Python 3 and pip.

COPY . /app: Copies the content of the current directory on your machine to the /app directory in the container.

RUN pip3 install --no-cache-dir -r requirements.txt: Installs Python dependencies listed in requirements.txt.

EXPOSE 80: Opens port 80, which can be used by your application to listen to web traffic.

CMD ["python3", "your_script.py"]: Runs the Python script your_script.py when the container starts.

Save the Dockerfile: Ensure that the Dockerfile and your Python application (e.g., your_script.py) are in the same directory. If you have dependencies, include a requirements.txt file in this directory as well.

Open a terminal: Navigate to the directory containing your Dockerfile using the terminal.

Build the Docker Image: Run the following command to build the Docker image:

docker build -t my-python-app .

-t my-python-app: Tags the image with the name my-python-app.
.: The dot indicates that the Dockerfile is located in the current directory.
Verify the Image: After the build completes, you can verify that the image was created successfully by listing your Docker images:

bash
Copy code
docker images
Example Output
bash
Copy code
REPOSITORY          TAG       IMAGE ID       CREATED          SIZE
my-python-app       latest    abc123def456   5 minutes ago    450MB
Running the Docker Image
After building the image, you can run a container using:

bash
Copy code
docker run -p 8080:80 my-python-app
This command maps port 80 in the container to port 8080 on your local machine, allowing you to access the app via http://localhost:8080 if it’s a web application.




After building and running your Docker image, you might want to consider the following steps depending on your use case:

1. Testing the Container:
Interact with Your Application: If it's a web application, you can access it via a browser at http://localhost:8080. If it’s a script or service, you may need to check its logs or output.
Check Logs: To view the logs of the running container, use:

docker logs <container_id>
You can find the container_id by listing all running containers with docker ps.

2. Debugging:
If your application isn’t behaving as expected, you might want to start the container in an interactive mode to debug:

docker run -it my-python-app /bin/bash
This command will give you a bash shell inside the container where you can manually run commands and inspect files.

3. Tagging the Image for Deployment:
If you plan to push this image to a Docker registry (like Docker Hub or AWS ECR), you should tag it properly:

docker tag my-python-app:latest myusername/my-python-app:v1.0
4. Pushing to a Docker Registry:
If you want to share your image or use it in a different environment, push it to a Docker registry:

docker login
docker push myusername/my-python-app:v1.0
Replace myusername with your Docker Hub username or the appropriate registry prefix.
5. Deploying the Container:
Local Deployment: Continue to run the container locally, possibly automating it with a docker-compose file for multi-container setups.
Cloud Deployment: Deploy the container to a cloud provider using services like AWS ECS, Azure AKS, Google Kubernetes Engine, etc.
Kubernetes: If you're using Kubernetes, create a Deployment and Service configuration to manage the container.

6. Cleaning Up:
Stop the container when you no longer need it:

docker stop <container_id>
Remove containers that are no longer running:
bash
Copy code
docker rm <container_id>
Remove the image if you don’t need it anymore:

docker rmi my-python-app
7. Automating with Docker Compose:
If your application requires multiple services (e.g., a database), you can define these in a docker-compose.yml file to manage them together.
Example docker-compose.yml:
yaml
Copy code
version: '3'
services:
  web:
    image: my-python-app:v1.0
    ports:
      - "8080:80"
    environment:
      - ENV=production
Run it using:
docker-compose up


##########################################################

Elk is a popular log collection and analysis tool commonly used in Kubernetes environments. ELK stands for Elasticsearch, Logstash, and Kibana. 
Elasticsearch is a search and analytics engine, Logstash is a data processing tool, and Kibana is a data visualization platform.
In Kubernetes, ELK is often used to collect and analyze logs from containerized applications. 
This helps developers and operators troubleshoot issues, monitor performance, and gain insights into application behavior. 
By integrating ELK into Kubernetes, teams can streamline log management and gain real-time visibility into their applications' behavior.
When deployed in Kubernetes, ELK can be used to collect, store, and analyze logs and metrics from various applications running in the Kubernetes cluster.

Why Use ELK in Kubernetes?
1.Centralized Log Management: ELK provides a centralized solution for collecting, storing, and analyzing logs from all applications running in a Kubernetes cluster.
2.Scalability: Both Kubernetes and ELK are designed to be highly scalable, making it easy to handle large volumes of log data.
3.Data Visualization: Kibana allows for real-time visualization of log data, helping developers and operators understand the state and performance of their applications.
4.Search and Analysis: Elasticsearch's powerful search capabilities enable quick retrieval and analysis of log data.
Helm is a package manager for Kubernetes that simplifies the deployment, management, and configuration of Kubernetes applications. 
It allows developers and operators to bundle Kubernetes manifests (e.g., Deployments, Services, ConfigMaps) into reusable and shareable packages called charts.
With Helm, users can create and share reusable templates for deploying applications, and manage dependencies and configurations in a centralized manner. 
This makes it a popular tool among Kubernetes developers and administrators.

Key Features of Helm
1.Application Packaging (Charts):
Helm bundles all Kubernetes resources required for an application into a chart, enabling easy sharing and deployment.

2.Declarative Management:
Helm simplifies managing applications as code, promoting Infrastructure as Code (IaC) practices.

3.Customization with Values:
Helm charts come with default configuration values (values.yaml), which can be overridden for customization.

4.Release Management:
Helm tracks application deployments (referred to as releases) and manages their lifecycle, including upgrades, rollbacks, and deletions.

5.Dependency Management:
Helm charts can include dependencies on other charts, making it easier to deploy complex applications.

#########################################################################################################
How Helm Works
1.Helm Chart:
A Helm chart is a collection of YAML templates that define Kubernetes resources and configuration files.

2.Helm Release:
When you deploy a Helm chart to your Kubernetes cluster, it becomes a release. Each release is an instance of the chart, 
allowing multiple deployments of the same application with different configurations.

3.Helm Repository:
Charts are stored in repositories (like Docker images in a container registry). Public repositories include ArtifactHub and Bitnami.
#############################################################################################################
Basic Helm Commands
1.Install a Chart: Deploy an application using a Helm chart:
helm install my-release my-chart

2.List Releases: View all active releases:
helm list

3.Upgrade a Release: Update an application with new configurations or a newer version of the chart:
helm upgrade my-release my-chart

4.Rollback a Release: Revert to a previous version if something goes wrong:
helm rollback my-release

5.Uninstall a Release: Remove an application and its Kubernetes resources:
helm uninstall my-release

##################################################################################################################
PK     ! 0(r     [Content_Types].xml (                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Tn0W?DV[0zlIQB
\"%كښl	w%=^i7+%g&0A6l4ingress in kubernetes

In Kubernetes, an Ingress is a centralized point of entry for incoming HTTP requests. 
It allows multiple services to be accessed through a single IP and port. 
The Ingress resource is responsible for routing incoming traffic to the relevant service, allowing for load balancing, SSL termination, 
and hostname-based routing. An Ingress controller forwards incoming requests to the corresponding service within the cluster.
This facilitates a scalable and maintainable way to manage incoming traffic in Kubernetes applications.

Kubernetes Ingress is an API object that helps developers expose their applications and 
manage external access by providing http/s routing rules to the services within a Kubernetes cluster.

In Kubernetes, Ingress is an API object that manages external access to services within a cluster, typically HTTP and HTTPS routes. 
It provides a way to expose multiple services over the same IP address and route traffic based on hostnames or paths. 
Essentially, Ingress acts as a more flexible and powerful alternative to other options like NodePort or LoadBalancer services.

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: default
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: example-service
            port:
              number: 80

###############################################

Egress:

In Kubernetes, egress refers to the outbound traffic originating from pods within the cluster to external resources or services, either within or outside the cluster
Egress traffic is used to access external services such as databases, APIs, and other services running outside of the cluster

Egress in Kubernetes refers to the flow of traffic from a pod or service in a cluster to an external network, such as the internet or another Kubernetes cluster. 
This can include accessing external services, making API calls, or communicating with other systems outside of the cluster.
Egress traffic is typically managed by Kubernetes' network policies, which control what traffic is allowed to exit the cluster.
By default, egress traffic is not restricted, but network policies can be used to limit or restrict egress traffic based on various criteria, such as IP addresses, ports, and protocols.


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-egress
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 203.0.113.0/24


###############################################
In Kubernetes, an Init Container is a special type of container that runs and completes its tasks before the main containers in a Pod start running.
Init containers are often used to set up or initialize the environment for the main application.
They can perform tasks such as pulling files, configuring settings, or ensuring certain conditions are met 
(like checking for a service dependency) before the primary containers begin execution.

1.Sequential Execution: Init containers run one after another in a specified order. A new Init container starts only after the previous one finishes successfully.
2.Failure Handling: If an Init container fails, Kubernetes will restart the Pod until the Init containers succeed. The main application container won't start until all Init containers complete successfully.
3.Different Image and Permissions: Init containers can use different images and configurations compared to the main containers. They can have different privileges or use tools that are not required by the main application.
4.Non-persistent State: Init containers are ephemeral, meaning they don't persist after the initialization is done, so any state or files created in an Init container should be passed to the main container through shared volumes.

apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'echo Waiting for service... && sleep 10']
  - name: init-db-setup
    image: busybox
    command: ['sh', '-c', 'echo Setting up database... && sleep 5']
  containers:
  - name: main-app
    image: nginx
    ports:
    - containerPort: 80
Kube-apiserver is responsible for authenticating, validating requests, retrieving and Updating data in ETCD key-value store. 
In fact kube-apiserver is the only component that interacts directly to the etcd datastore.
The other components such as kube-scheduler, kube-controller-manager and kubelet uses the API-Server to update in the cluster in their respective areas.

The Kube API Server is a primary component of the Kubernetes control plane, responsible for managing and exposing the Application Programming Interface (API) for cluster resources.
It exposed the Kubernetes API to users and accepts CRUD (Create, Read, Update, Delete) operations from clients. 
The API Server authenticates and authorizes users, validates and processes requests, and updates the etcd database.
It's the entry point for most users and systems interacting with the cluster, allowing them to manage and manipulate cluster resources such as pods, services, and deployments

Handling API Requests: The API Server processes requests from clients (like kubectl, controllers, or other components) and performs the corresponding actions on the cluster. 
These requests could be about creating, updating, deleting, or retrieving resources like Pods, Services, Deployments, etc.

Cluster State Management: The API Server maintains the state of all cluster resources by interacting with etcd, the distributed key-value store. 
When changes occur (e.g., a new pod is created), the API Server updates the state in etcd, ensuring consistency and availability of cluster data.

The API Server can authenticate users and services using tokens, certificates, or other mechanisms.
After authentication, the API Server enforces RBAC (Role-Based Access Control) to ensure that only users or services with the correct permissions can access specific resources.
API Server acting as the interface for managing cluster data stored in etcd.

Kubernetes API Server Workflow
Here’s a simplified overview of how the Kubernetes API Server works:

Client Request: A user (or a system component like the Scheduler or Controller Manager) sends an API request (via kubectl or another client) to the API Server.

Authentication: The API Server authenticates the request using tokens, certificates, or other authentication mechanisms.

Authorization: The API Server checks the user’s permissions through RBAC to see if they are allowed to perform the requested action.

Admission Control: The request is passed through admission controllers that validate or modify the request (e.g., applying security policies or limiting resource requests).

Action Execution: The API Server performs the action, such as creating, updating, or deleting a resource. It interacts with etcd to store the new state.

Response: The API Server sends a response back to the client with the outcome of the request, which may include data about the resource or a success/error message.

Kubernetes API Server Components
Kubelet: The Kubelet is responsible for managing the lifecycle of containers on each node. It communicates with the API Server to get the current state and desired state of Pods and then takes action to reconcile the two.

Controllers: Controllers monitor the state of the cluster and make changes to ensure that the desired state matches the actual state. The API Server provides the interface for controllers to update resources in the cluster (e.g., ReplicaSet, Deployment, etc.).

Scheduler: The Scheduler is responsible for assigning Pods to nodes based on available resources. The Scheduler interacts with the API Server to get the list of available nodes and schedules Pods to those nodes.

etcd: A distributed key-value store that holds the cluster’s state, including information about Pods, Services, ConfigMaps, and other resources. The API Server reads and writes data to etcd to maintain the cluster state.


The Kube Controller Manager is a critical component of the Kubernetes control plane. 
Its primary responsibility is to run and manage controllers, which are responsible for enforcing the desired state of the cluster. 
These controllers manage various aspects of the cluster, such as node health, pod scheduling, and persistent storage. 
The Controller Manager is responsible for creating and managing these controllers, ensuring they are running and functioning correctly.
It also handles communication between controllers and other components, like the API Server, to ensure the cluster remains in a stable and secure state.

In kubernetes terms, a controller is a process that continuously monitors the state of the components within the 
system and works towards bringing the whole system to the desired functioning state.

Node Controller
Responsible for monitoring the state of the Nodes and taking necessary actions to keep the application running.

Replication Controller
It is responsible for monitoring the status of replicasets and ensuring that the desired number of pods are available at all time within the set.

kube-scheduler is responsible for scheduling pods on nodes.
The kube-scheduler is only responsible for deciding which pod goes on which node. 
It doesn't actually place the pod on the nodes, that's the job of the kubelet.

The Kube Scheduler is a primary component of the Kubernetes control plane that responsible for assigning containerized applications to available resources in a cluster.
It is often referred to as the "traffic cop" for Kubernetes. 
The Scheduler ensures that Pods are created on a suitable node with the necessary resources and availability.
Upon receiving a Pod creation request, the Scheduler calculates a list of suitable nodes and assigns the Pod to the most suitable node based on factors such as resource availability, node affinity, and anti-affinity.




kubectl get all --->to get all info about present ns

kubectl get cluster-info --->to get info about cluster

kubectl get nodes ---> available pods nodes

kubectl get pods ---> available pods

kubectl get pods --all-namespaces ---> availble pods in all ns

k get pods --selector env=dev ---> to check how many pods exist in dev env 

kubectl get pods webapp-daemonset-79m8x -o jsonpath='{.spec.containers[*].name}' ---->To get a container name inside a pod

k get all --selector env=prod ---> To get all pods ,ns,replicasets in env=prod

k get all --selector env=prod,bu=finance,tier=frontend  ----> POD which is part of the prod environment, the finance BU and of frontend tie

kubectl describe pod podName ---> info about specific pod

kubectl apply -f pod.yml ---> create pod

kubectl delete pod podName ---> to delete a pod

kubectl logs podName ---> to get logs about pod

kubectl exec -it podName -- /bin/bash/ --->execute command inside a pod

kubectl get deployments ---> get all deployments

kubectl rollout status deployment/deploymentName --->rollback to previous version

kubectl rollout undo deployment/deployment name ---> roll back to previous version of dep

kubectl delete deployment deployment name

kubectl get ns ---> availble namespaces

kubectl create namespace namespacename

kubectl get pods -n namespace name --->Run a command in specific namespace

kubectl create configmap configmap-name --from-literal=key=value 

kubectl create secrete generic secrete name --from-literal=key=value

kubectl describe resourcetype resourcename --->info about resource

kubectl logs podNmae

kubectl logs podName -c containerName ---> logs in container in a pod

kubectl get pv

kubectlctl get pvc

kubectl delete pv pv name


kubectl run nginx --image=nginx123 --dry-run=client -o yaml ---directly it will create yaml file

kubectl get roles -n namespace ---> get all roles in namespaces

kubectl apply -f role.yml

kubectl exec -it my-pod -- /bin/bash ---> very ueful command for troubleshooting

kubectl scale rs replicasetName --replicas=5 --->update our pods to 5 pods in rs

kubectl edit rs rsName ---> edit pods in yaml file

kubectl create deployment my-deployment --image=nginx --replicas=3 --->create deployment by using imperative type

kubectl set image deployment/my-deploy nginx=nginx:1.18-perl --record  ---> we can set or edit image in deplyments

kubectl replace -f replicasset.yml 

kubectl scale --replicas=6 -f replicaset.yml ---> to update replicas

kubectlctl scale --replicas=6 replicasset my-app replicaset --->

kubectl get pods --namespace=kube-system --> to see the pods in the another namespace

kubectl create -f pod.yml --namespace=dev ---> create a namespace in another namespace

kubectl config set-context --current --namespace=<namespace> ---> to switch to another namespace

kubectl run redis --image=redis -n sainamespace ---> create a pod in sainamespace

kubectl set image deployment nginx nginx=nginx:1.18

kubectl scale deployment nginx --replicas=5

kubectl expose deployment nginx --port 80

kubectl taint nodes nodeName key=value:NoSchedule --->applied taint on a node

kubectl taint node nodeName taint status in describe command to add finally -  -->untainted

kubectl label node node01 coor=blue ---> labeled

kubectl create deployment blue --image=nginx --replicas=3 ---> deployment created

git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git --->used to download metric server from the git

cd kubernetes-metrics-server ---> change dir to 

kubectl create -f . ---> deploy metrics server by creating all the components downloaded

kubectl top node  ---> metrics server to start gathering data.

kubectl top node ---> used to see the metrics of node

kubectl get pod  ---> used to see the metrics of pod

kubectl create configmap \ configmapName --from-literal=key=value \ --from-literal=app_color=blue --->created by using literal

kubectl create configmap my-config --from-file=path/to/configfile ---> created from a file

kubectl create secret generic my-secret --from-literal=username=admin --from-literal=password='s3cr3t' ---> create a secret from a literal

kubectl create secret generic my-secret --from-file=path/to/secretfile --> create a secrete from a file

kubectl replace --force -f yaml file ---> to delete a pod and replace a new changes 

k describe cm configmapName --> to see the info of a cm

kubectl -n elastic-stack logs kibana --->by inspecting kibana logs

kubectl drain nodeName --ignore-daemonsets --->Draining a node moves all the running pods to other nodes in the cluster, preparing the node for maintenance

kubectl uncordon nodeName ---> The maintenance tasks have been completed. Configure the node node01 to be schedulable again,After upgrading and rebooting, bring the node back into service using the kubectl uncordon

kubectl cordon nodeName --> Kubernetes is used to mark a node as unschedulable, meaning that no new pods can be scheduled on that node.

kubeadm upgrade plan ---> to check latest stable version for upgrade

sudo kubeadm upgrade apply version name ---> to apply the version

kubectl describe pod etcd-controlplane  -n kube-system --->to check version of ETCD running on the cluster

kubectl config get-clusters ---> to check no of clusters

kubectl config use-context cluster1 ---> switched to cluster-1

openssl genrsa -out tls.key 2048 --->to create private key

openssl req -new -key tls.key -out tls.csr --->create certificate signing request(CSR)

openssl x509 -req -in tls.csr -signkey tls.key -out tls.crt -days 365  --->self sign the certificate

ip netns add Red ---> create network namespace

ip netns ---> show netns

netstat -npl | grep -i schedular  ---> to check schedular listening on which port

ip address ---> network interface for cluser connecivity

ip address show eth0--->MAC address of the interface on the controlplane,eth0 is the network interface

ssh node01 ---> to change to another node

ip address show type bridge ---> to check interface or bridge in this host

ip route --->to check routes

ip link ---> The ip link command in Kubernetes is typically used within the container or pod to view or  manage network interfaces.

nslookup google.com ---> nslookup is a command-line tool used to query the Domain Name System (DNS) to obtain domain name or IP address mapping details

dig --->dig (Domain Information Groper) is a powerful command-line tool for querying DNS servers and obtaining detailed information about domain name resolution






controlplane $ vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-api-deployment
  labels:
    app: my-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-api
  template:
    metadata:
      labels:
        app: my-api
    spec:
      containers:
      - name: my-api-container
        image: my-api-image:latest
        ports:
        - containerPort: 8080



controlplane $ kubectl create -f deploy.yml
deployment.apps/api-deployment created
controlplane $ kubectl get pods

NAME                              READY   STATUS    RESTARTS   AGE
api-deployment-867d7679cd-cz9n5   1/1     Running   0          12s
api-deployment-867d7679cd-n5kst   1/1     Running   0          12s
api-deployment-867d7679cd-vtzw6   1/1     Running   0          12s

Exposing Your API with a Service
To make your API accessible, you need to expose it using a Kubernetes Service.

vi api-service.yml

apiVersion: v1
kind: Service
metadata:
  name: my-api-service
spec:
  selector:
    app: my-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer


kubectl create -f api-service.yml


controlplane $ kubectl api-resources
NAME                                SHORTNAMES   APIVERSION                        NAMESPACED   KIND
bindings                                         v1                                true         Binding
componentstatuses                   cs           v1                                false        ComponentStatus
configmaps                          cm           v1                                true         ConfigMap
endpoints                           ep           v1                                true         Endpoints
Manual scheduling in Kubernetes refers to the process of directly assigning a Pod to a specific node in the cluster without relying on Kubernetes' default scheduler. 
This approach bypasses the automated decision-making of the scheduler and is useful for testing, debugging, or scenarios where precise control is required.

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: node1
The Pod will be scheduled directly on the node named node1.
If node1 is unavailable or does not have sufficient resources, the Pod will remain in a Pending state.
In Kubernetes, a multiple scheduler refers to the ability to run multiple scheduler components simultaneously to manage and schedule containerized applications. 
This allows for more efficient resource utilization, improved scalability, and increased flexibility. With multiple schedulers, 
you can configure different scheduling strategies for different workloads, such as using one scheduler for batch jobs and another for stateful applications. 
Additionally, multiple schedulers can help improve high availability and redundancy by providing a fallback option in case one scheduler becomes unavailable.

Some common use cases for multiple schedulers in Kubernetes include:
Running a primary scheduler for production workloads and a secondary scheduler for development or testing environments
Using one scheduler for stateless applications and another for stateful applications
Implementing a canary deployment strategy using multiple




A Namespace  is a logically portion of the cluster. 
It provides a way to logically separate and organize resources such as Pods, Services, Deployments, and other Kubernetes objects. 
This is especially useful for environments with multiple teams, projects, or applications running within the same Kubernetes cluster.
Separate resources of different teams, projects, or environments (e.g., development, staging, production).
Apply Role-Based Access Control (RBAC) policies to restrict users' access to specific namespaces.

Using Namespace using YAML:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
spec:
  containers:
  - name: nginx
    image: nginx


controlplane $ kubectl get namespaces      
NAME                 STATUS   AGE
default              Active   15d
kube-node-lease      Active   15d
kube-public          Active   15d

controlplane $ kubectl create namespace sainamespace
namespace/sainamespace created
controlplane $ kubectl get namespaces
NAME                 STATUS   AGE
default              Active   15d
kube-node-lease      Active   15d
kube-public          Active   15d
sainamespace         Active   5s

switch to another namespace:
kubectl config set-context --current --namespace=my-namespace

controlplane $ vi namespace.yml
controlplane $ kubectl create -f namespace.yml
pod/my-nginx-pod created
controlplane $ kubectl get pods
No resources found in default namespace.
controlplane $ kubectl get pods -n sainamespace
NAME           READY   STATUS    RESTARTS   AGE
my-nginx-pod   1/1     Running   0          44s
controlplane $ kubectl get all --all-namespaces
NAMESPACE            NAME                                           READY   STATUS    RESTARTS      AGE
kube-system          pod/calico-kube-controllers-75bdb5b75d-2b6mr   1/1     Running   1 (23m ago)   15d
kube-system          pod/canal-wrxsq                                2/2     Running   2 (23m ago)   15d
kube-system          pod/coredns-5c69dbb7bd-5rv7t                   1/1     Running   1 (23m ago)   15d
sainamespace         pod/my-nginx-pod                               1/1     Running   0             4m


controlplane $ kubectl get all -n kube-system 
NAME                                           READY   STATUS    RESTARTS      AGE
pod/calico-kube-controllers-75bdb5b75d-2b6mr   1/1     Running   1 (27m ago)   15d
pod/canal-wrxsq                                2/2     Running   2 (27m ago)   15d
pod/coredns-5c69dbb7bd-5rv7t                   1/1     Running   1 (27m ago)   15d


NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   15d

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/canal        1         1         1       1            1           kubernetes.io/os=linux   15d
daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   15d

NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/calico-kube-controllers   1/1     1            1           15d
deployment.apps/coredns                   2/2     2            2           15d

NAME                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/calico-kube-controllers-75bdb5b75d   1         1         1       15d
replicaset.apps/coredns-5c69dbb7bd                   2         2         2       15d
replicaset.apps/coredns-7db6d8ff4d                   0         0         0       15d
controlplane $ kubectl get ns
NAME                 STATUS   AGE
default              Active   15d
kube-node-lease      Active   15d
kube-public          Active   15d
kube-system          Active   15d
local-path-storage   Active   15d
sainamespace         Active   13m
controlplane $ kubectl config set-context --current --namespace=default
Context "kubernetes-admin@kubernetes" modified.
controlplane $ kubectl get pods
No resources found in default namespace.
controlplane $ kubectl config set-context --current --namespace=sainamespace
Context "kubernetes-admin@kubernetes" modified.

controlplane $ kubectl get pods
NAME           READY   STATUS    RESTARTS   AGE
my-nginx-pod   1/1     Running   0          11m
controlplane $ kubectl get pods -A
NAMESPACE            NAME                                       READY   STATUS    RESTARTS      AGE
kube-system          calico-kube-controllers-75bdb5b75d-2b6mr   1/1     Running   1 (35m ago)   15d
kube-system          canal-wrxsq                                2/2     Running   2 (35m ago)   15d
sainamespace         my-nginx-pod                               1/1     Running   0             16m
controlplane $ 
in Kubernetes, a Network Policy is a resource that allows you to define rules that govern how pods communicate with each other and with other network endpoints.
It works by controlling which traffic is allowed to flow between pods and external systems, effectively controlling the communication at the network level.

Namespace Scope: Network policies apply to pods within a namespace, but you can define policies that allow or block traffic between pods in different namespaces.
Pod Selector: Network policies use labels to select which pods the rules apply to.
Ingress and Egress: You can define rules for both inbound (ingress) and outbound (egress) traffic. 
For example, you can restrict incoming traffic to only allow specific sources or restrict outgoing traffic to certain destinations.

Pod Selector: Specifies which pods the network policy applies to.
Ingress Rules: Defines rules about what incoming traffic is allowed into the selected pods.
Egress Rules: Defines rules about what outgoing traffic is allowed from the selected pods.
Policy Types: Specifies whether the policy applies to ingress, egress, or both.

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-only-internal
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: backend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
  egress:
  - to:
    - podSelector:
        matchLabels:
          role: database


Use Cases for Network Policies
Isolate Sensitive Services: For example, you can restrict access to database pods to only specific backend pods.
Restrict Internet Access: Egress policies can block pods from communicating with external internet addresses, enforcing internal-only communication.
Security Compliance: Enforce security boundaries and ensure that only authorized communication between microservices is possible.
controlplane $ vi pod.yml
controlplane $ kubectl create -f pod.yml
error: error validating "pod.yml": error validating data: apiVersion not set; if you choose to ignore these errors, turn validation off with --validate=false
controlplane $ vi pod.yml
controlplane $ kubectl create -f pod.yml
pod/myapp-pod created
controlplane $ kubectl get pods
NAME        READY   STATUS              RESTARTS   AGE
myapp-pod   0/1     ContainerCreating   0          7s
controlplane $ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          23s
controlplane $ vi service.yml
controlplane $ kubectl create -f service.yml
Error from server (BadRequest): error when creating "service.yml": service in version "v1" cannot be handled as a Service: no kind "service" is registered for version "v1" in scheme "pkg/api/legacyscheme/scheme.go:30"
controlplane $ vi service.yml
controlplane $ kubectl create -f service.yml
service/app-service created
controlplane $ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          8m
controlplane $ kubectl get svc
NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
app-service   NodePort    10.107.194.155   <none>        80:30020/TCP   4m6s
kubernetes    ClusterIP   10.96.0.1        <none>        443/TCP        15d
controlplane $ kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   15d   v1.30.0
controlplane $ kubectl get nodes -o wide
NAME           STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
controlplane   Ready    control-plane   15d   v1.30.0   172.30.1.2    <none>        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.13
controlplane $ curl http://172.301.2:30020
curl: (6) Could not resolve host: 172.301.2
controlplane $ curl http://172.30.1.2:30020
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
controlplane $ 







vi deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: nginx
        ports:
        - containerPort: 80




vi service.yml



# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30020
  type: NodePort # or ClusterIP, NodePort, etc.


DNS:Domain name system


There are Four Types of services :
ClusterIP. Exposes a service which is only accessible from within the cluster.
NodePort. Exposes a service via a static port on each node's IP.
LoadBalancer. Exposes the service via the cloud provider's load balancer.
ExternalName.


1.ClusterIP (default)

Description: Exposes the service on an internal IP in the cluster. This type makes the service accessible only within the cluster.
Use Case: Ideal for internal communication between microservices within the cluster.

apiVersion: v1
kind: Service
metadata:
  name: my-clusterip-service
spec:
  type: ClusterIP
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
#############################

2.NodePort

Description: Exposes the service on each Node's IP at a static port (the NodePort). A ClusterIP service, to which the NodePort service routes, is automatically created.
Use Case: Useful for exposing the service to external traffic using <NodeIP>:<NodePort>.

apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  type: NodePort
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
      nodePort: 30007
###################################

3. LoadBalancer
Exposes the service externally using a cloud provider's load balancer.
A ClusterIP service, to which the external load balancer routes, is automatically created.
Use case: Ideal for production environments where you need to expose your service to the internet.


apiVersion: v1
kind: Service
metadata:
  name: my-loadbalancer-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
  type: LoadBalancer
####################################

4.ExternalName
Maps the service to the contents of the externalName field (e.g., my.database.example.com).
The externalName must be a valid DNS name.
Does not create a cluster IP.
Use case: When you want to proxy a service to an external DNS name.

apiVersion: v1
kind: Service
metadata:
  name: my-externalname-service
spec:
  type: ExternalName
  externalName: my.database.example.com
#######################################

5.Headless Service
Does not allocate a ClusterIP.
Service discovery is done directly via DNS (i.e., DNS A or SRV records) or through a custom mechanism.
Use case: Useful for stateful applications, where each pod needs to be addressed individually.

apiVersion: v1
kind: Service
metadata:
  name: my-headless-service
spec:
  clusterIP: None
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
#########################################



Node affinity is a feature in Kubernetes that allows you to specify an affinity between a pod and a node. 
This feature is particularly useful when you want to deploy pods to specific nodes based on certain criteria.
Node affinity is similar to pod affinity, but it operates at the node level instead of the pod level. 
You can use node affinity to deploy pods to nodes with specific hardware or software configurations,
such as a certain amount of CPU or memory, or a specific device driver.

Node affinity in Kubernetes is a way to constrain which nodes a pod can be scheduled on, based on node labels.
It is similar to nodeSelector, but offers more flexibility and control. 
Node affinity allows you to specify rules about which nodes a pod can or cannot run on,
by matching the pod’s scheduling preferences with node labels.

There are two types of node affinity in Kubernetes:

1. Required During Scheduling, Ignored During Execution (requiredDuringSchedulingIgnoredDuringExecution):

Purpose: This is a hard requirement. The scheduler ensures that the pod is only scheduled on nodes that satisfy the affinity rules.
Behavior: If no suitable node is available, the pod won’t be scheduled

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: "disktype"
          operator: In
          values:
          - ssd

This example requires the pod to be scheduled only on nodes with the label disktype: ssd.

2. Preferred During Scheduling, Ignored During Execution (preferredDuringSchedulingIgnoredDuringExecution):

Purpose: This is a soft preference. The scheduler tries to place the pod on nodes that satisfy the affinity rules, but it's not mandatory.
Behavior: If no matching nodes are available, the scheduler will still schedule the pod on other available nodes.


affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: "zone"
            operator: In
            values:
            - east
This example expresses a preference to run the pod on nodes labeled with zone: east, but it can still be scheduled elsewhere if no such node is available.


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "disktype"
            operator: In
            values:
            - ssd
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: "zone"
            operator: In
            values:
            - east
This pod will only be scheduled on nodes with disktype: ssd, and it prefers nodes with zone: east
Node selector:

A node selector in Kubernetes is a key-value pair that is used to schedule pods on specific nodes in a cluster. 
It allows you to label nodes with specific attributes and then use those labels to select which nodes a pod should run on.
This is useful for a variety of use cases, such as scheduling a pod on a node with a specific amount of memory or CPU, or 
scheduling a pod on a node with a specific operating system. Node selectors are defined as part of a pod's configuration and 
can be used in combination with other scheduling options to ensure that your pods are deployed to the right nodes.

Node selectors in Kubernetes allow you to control which nodes your pods are scheduled on by defining specific label-based constraints. 
By using node selectors, you can tell Kubernetes to run a pod only on nodes that match certain labels.

kubectl label nodes node1 size=Large


apiVersion: v1
kind: Pod
metadata:
  name: frontend-pod
spec:
  containers:
  - name: frontend
    image: frontend-image
  nodeSelector:
    size: Large

The nodeSelector field specifies key-value pairs (e.g., size=Large).
Kubernetes will only schedule the pod on nodes with both labels.


###########################################################################################################
Prometheus and Grafana are two popular open-source tools used in monitoring and observability.
Prometheus is a monitoring system and time series database that collects metrics from applications and services. 
It provides a query language (PromQL) for retrieving and aggregating data. 
Grafana, on the other hand, is a visualization platform that uses Prometheus data to create dashboards, graphs, and charts. 
While Prometheus focuses on collecting and storing data, Grafana focuses on visualizing and analyzing that data.

It’s a powerful tool for creating real-time monitoring dashboards.
Grafana connects to Prometheus as a data source and provides a flexible interface to visualize those metrics through dashboards.
Together, they form a powerful combination for monitoring and observability, especially in cloud-native environments like Kubernetes.
In short, Prometheus is the data collector, and Grafana is the data viewer.

Why is Observability Important?
Proactive Monitoring: Detect and respond to issues before they impact users.
Faster Troubleshooting: Identify the root cause of problems using detailed logs and traces.
Performance Optimization: Analyze trends to optimize system performance and resource utilization.
Business Insights: Understand the overall health of the system and its impact on business metrics like SLAs or SLOs.
Key Tools for Observability:
Prometheus (Metrics)

For collecting and querying time-series data, such as CPU and memory usage.
Grafana (Visualization)

For visualizing data from various sources, including Prometheus, Elasticsearch, and others.
Elasticsearch + Kibana (Logs)

Elasticsearch for indexing and storing logs, Kibana for visualizing and analyzing them.
Jaeger or Zipkin (Tracing)

##########################################################################
Initialising Kubernetes... done

controlplane $ vi pv.yml
controlplane $ kubectl create -f pv.yml
persistentvolume/example-pv created
controlplane $ kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
example-pv   5Gi        RWO            Retain           Available                          <unset>                          6s
controlplane $ vi pvc.yml
controlplane $ kubectl create -f pvc.yml
persistentvolumeclaim/example-pvc created
controlplane $ kubectl get pvc
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
example-pvc   Pending                                      local-path     <unset>                 10s
controlplane $ kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
example-pv   5Gi        RWO            Retain           Available                          <unset>                          2m33s
controlplane $ vi pvc.yml





apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: "/mnt/data"

capacity: Specifies the size of the storage, in this case, 10Gi.
accessModes: Defines how the volume can be accessed. ReadWriteOnce means it can be mounted as read-write by a single node.
persistentVolumeReclaimPolicy: Determines what happens to the PV when the PVC is deleted.
Retain means the data is retained after the PVC is deleted.
storageClassName: Associates the PV with a storage class. If you don’t want to use storage classes, you can omit this or set it to "manual".
hostPath: Defines the actual storage location on the host machine for testing purposes.
In production, you might use other storage types like NFS, AWS EBS, GCE PD, etc.




apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

Use the PVC in a Pod
Once the PVC is bound to a PV, you can use the PVC in a pod to mount the storage.

#######################################################
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: nginx
    volumeMounts:
    - mountPath: /usr/share/nginx/html
      name: my-storage
  volumes:
  - name: my-storage
    persistentVolumeClaim:
      claimName: my-pvc



accessModes: Should match one of the access modes specified in the PV.
resources: Specifies the amount of storage requested. This should be equal to or less than the capacity of the PV.
storageClassName: This should match the storageClassName defined in the PV.
If storageClassName is not specified, it will bind to a PV without a storage class or to the default storage class.

#####################################################################

pv in kubernetes

PV stands for Persistent Volumes in Kubernetes. 
It is a resource that provides persistent storage to pods.
PVs are managed by the cluster and are decoupled from pods, ensuring that when a pod is deleted or recreated, the data stored in the PV is preserved.
PVs can be provisioned statically by cluster administrators or dynamically through storage classes.
They can be attached to pods using Persistent Volume Claims (PVCs) and can be used for storing applications' data, logs, or other persistent data.

pvc in kubernetes

PVC (Persistent Volumes Claim) is a Kubernetes feature that allows you to request storage resources from a cluster.
A PVC is a request for storage that is provisioned and managed by a cluster administrator. 
Once a PVC is created, it can be bound to a Persistent Volume (PV), which is a resource in the cluster 
that represents a piece of networked storage. PVCs provide a way to decouple storage from the deployment of an application, 
allowing for more flexibility and scalability in the deployment of stateful applications.

PVs can be provisioned through the statically or dynamically.
static provisioning involves creating PVs in advance,
while dynamic provisioning allows Pvs to be created on-demand when requested by the application.

Dynamic Provisioning of PVs facilitated through the use of storage classes

when pvc created,kubernetes dynamically binds it to an available pv that matches the requirements specified in the claim


storage class in kubernetes

In Kubernetes, a storage class is a way to request storage with specific characteristics, such as type and size, without explicitly specifying the underlying Persistent Volumes (PVs).
When a Pod requests storage, Kubernetes creates a Persistent Volume Claim (PVC) and binds it to a matching PV.
This decouples the application from the storage infrastructure, allowing for easier scaling and flexibility. 
Storage classes can be used to provision storage from various sources, such as local disks, NFS, or object storage like Amazon S3. 
They provide a standardized way to interact with storage resources in Kubernetes.
Kubelet:

The kubelet is a critical component of the Kubernetes ecosystem, responsible for managing and running containers on a machine. 
It is a service that runs on every machine (node) in your cluster.

Kubelet is a key component of Kubernetes that manages containers and nodes in a cluster: 
Communication: Kubelet acts as a node-level agent that communicates between the Kubernetes control plane and individual nodes. 
Container management: Kubelet is responsible for container execution, resource management, and monitoring container health. 
Container lifecycle: Kubelet manages container lifecycles. 
Node status: Kubelet reports back to the control plane about the node's status. 
Pod management: Kubelet creates, destroys, or updates pods and their Docker containers. 
Worker node registration: Kubelet is responsible for registering worker nodes in the Kubernetes cluster. 
Discover Kubelet, and Why it's Important for Kubernetes ...
Kubelet runs on each node of a cluster. It takes orders from the master node and transfers them to the worker nodes, which then execute the tasks.

######################################################

Etcd is a distributed key-value store that serves as a low-level storage mechanism for certain Kubernetes components, such as the cluster's state.
It's used to store and manage cluster data, including node information, pod and service configurations, and persistent volume claims. 
Etcd provides a consistent and fault-tolerant storage solution for Kubernetes, allowing it to scale and operate reliably in large, distributed environments. 
It's a lightweight, flexible, and highly available solution that supports a wide range of data structures and makes it easy to store and retrieve cluster data.

etcd role in Kubernetes
The etcd stores all cluster information, such as its current state, desired state, configuration of resources, and runtime data.
So DevOps can run single-node or multi-node clusters as etcd clusters, which interact with Kubernetes' etcd API to manage the clusters.

In Kubernetes, etcd is a key-value store that manages configuration data, state data, and metadata for the cluster. 
It's a critical component of the Kubernetes platform that ensures the system's reliability and consistency.
etcd is responsible for storing and managing cluster state, including node identities, pod locations, and deployment configurations.

###########################################################

kubeadm in kubernetes

Kubeadm is a tool used to set up and manage Kubernetes clusters. 
Developed by the Kubernetes project, kubeadm provides a simple and efficient way to deploy and manage clusters, particularly for large-scale deployments. 
With kubeadm, you can create a Kubernetes master node and join worker nodes to the cluster.
It also helps to manage node roles, network policies, and SSL certificates. 
Additionally, kubeadm supports various clustering scenarios, including HA (High Availability) and OpenShift deployments. 
Its ease of use and flexibility make it a popular choice for setting up and managing Kubernetes clusters.

###########################################################
Difference between Docker and Containerd?

Docker:
Docker is a full containerization platform that simplifies the creation, management, and orchestration of containers.
It includes a set of tools like the Docker CLI (Command-Line Interface), Docker Engine, and Docker Hub.
Docker also handles building images, networking, volumes, and more. 
It provides an end-to-end platform for developers to create and manage containers easily.

containerd:
containerd is a lower-level container runtime. It is responsible for the core operations of containers, such as starting, stopping, and managing container lifecycles.
containerd is primarily focused on the container runtime aspect, and it does not include features for building container images or managing complex orchestration tasks.

##############################################################

Kube-Proxy is a network proxy that runs on each node in a Kubernetes cluster.
It is responsible for maintaining network connectivity between services and pods. 
Kube-Proxy does this by translating service definitions into actionable networking rules.

Kube-proxy is a daemon that runs on each node in a Kubernetes cluster, responsible for making incoming traffic reach the desired service.
It's a network proxy that sits between the userspace and the Linux network stack, intercepting and forwarding traffic to the service's pod. 
Kube-proxy uses IPTables or IPVS (depending on the platform) to redirect traffic to the service.
It also handles load balancing, circuit breaking, and other networking features.
Kube-proxy is an essential component of Kubernetes, enabling communication between pods and services.

Kube-proxy is a networking component in Kubernetes that manages traffic routing and communication between services and pods within a cluster

####################################################################

imperative
The user is responsible for providing explicit instructions to Kubernetes on how to manage resources and applications.
This method is useful for quick changes and ad-hoc management, but it can be error-prone and difficult to maintain.

Declarative
The system is responsible for automatically applying the desired state of the resources and applications to the cluster. 
This method is generally preferable for production use cases because it enables version control, reviewable changes, and reproducibility.

For example, using kubectl run is imperative, as you're explicitly telling Kubernetes to create a pod.
In contrast, using a YAML file to describe a pod's configuration is declarative, as you're defining the desired state, and Kubernetes creates the pod accordingly.

The imperative method is easy to use for ad-hoc operations, while the declarative approach promotes infrastructure-as-code and is better suited for production systems

##########################################################

The kubectl cordon command in Kubernetes is used to mark a node as unschedulable, meaning that no new pods can be scheduled on that node,
but it does not affect the existing pods running on the node.
This is useful when you want to prepare a node for maintenance (e.g., for upgrades or troubleshooting) but do not want to disrupt the currently running workloads.

###############################################################

The kubectl uncordon command is used in Kubernetes to mark a node as schedulable again after it has been drained or cordoned. 
When a node is cordoned, it is marked as unschedulable, meaning no new pods can be scheduled on it. Once you’ve finished maintenance or updates (such as OS upgrades), 
you can use the uncordon command to make the node available for scheduling again.

###################################################################

The kubectl drain command is used in Kubernetes to prepare a node for maintenance by safely evicting all running pods from the node.
This command ensures that workloads are moved to other nodes in the cluster and the node is ready for upgrades, 
configuration changes, or other administrative tasks.

####################################################################


In Kubernetes, Role-Based Access Control (RBAC) is a method of controlling access to cluster resources based on a user's identity and role. 
It allows administrators to define roles and bind them to users or service accounts, granting specific permissions and privileges. 
RBAC provides a way to manage fine-grained access control within the cluster, ensuring that users only have access to resources and actions they need to perform their tasks.
RBAC can be used to enforce policies, monitor activity, and maintain the overall security and integrity of the cluster.
Role-based access control (RBAC) is a method of protecting sensitive data from improper access, modification, addition, or deletion. 
It allows employees to have access to the information required to fulfill their responsibilities.
Role-based access control (RBAC) objects determine whether a user is allowed to perform a given action within a project. 
Cluster administrators can use the cluster roles and bindings to control who has various access levels to the OpenShift Container Platform platform itself and all projects.
####################

Role:
###############
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
#####################################
API group with an empty string ("") refers to the core API.
Resources in the core API include:
pods
services
nodes
configmaps
namespaces

Named API Groups:

Non-core resources are part of named API groups.
Named groups help modularize Kubernetes by separating features into logical domains.
Example API groups include:
apps (e.g., Deployments, StatefulSets)
rbac.authorization.k8s.io (e.g., Roles, RoleBindings)
batch (e.g., Jobs, CronJobs)

########################
The next step is to link the user to that role.
For this we create another object called RoleBinding. This role binding object links a user object to a role.
Bind The Role to the user:
#######################
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: default
subjects:
- kind: User
  name: "jane"  # This is the user to whom you're giving permissions
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

############################
View RBAC:

To list roles
 kubectl get roles
To list rolebindings
kubectl get rolebindings
kubectl describe rolebinding read-podsbinding --->describe the role binding

Replication Controller is the older technology that is being replaced by a ReplicaSet.
ReplicaSet is the new way to setup replication.In Kubernetes, a ReplicaSet is a higher-level abstraction over a ReplicationController. 

 Its primary function is to maintain a stable set of replica pods, automatically replacing any that fail or are deleted.

A ReplicaSet's configuration defines the number of Pods required, and if a Pod fails or is evicted, it creates more Pods to make up for the loss. 
Scaling
You can scale an application up or down by changing the number of replicas in the configuration file or using the command line. 
It ensures that a specified number of replicas (identical copies) of a Pod are running at any given time.

A ReplicaSet automatically handles Pod management, including creating, scaling, and restarting pods, 
to maintain the desired level of replicas. This allows developers to define the desired state of their application and the Kubernetes
control plane ensures that the state is maintained. ReplicaSets provide greater flexibility and scalability compared 
to ReplicationControllers, making them a preferred choice for managing production workloads.

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-replicaset
spec:
  replicas: 3  # Number of pod replicas
  selector:
    matchLabels:
      app: my-app  # Label to match
  template:
    metadata:
      labels:
        app: my-app  # Label to apply to pods
    spec:
      containers:
      - name: my-container
        image: nginx







In Kubernetes, resource requests and resource limits are used to manage the CPU and memory resources allocated to containers in a cluster.

1. Resource Requests
A resource request specifies the minimum amount of resources that a container is guaranteed to have available to it. 
When a container is scheduled onto a node, Kubernetes ensures that the node has at least the requested amount of CPU and memory available.
If a node doesn’t have the requested resources, the container won't be scheduled there.

resources:
  requests:
    memory: "256Mi"
    cpu: "500m"
This means the container requests at least 256Mi of memory and 0.5 CPU cores.

2. Resource Limits
A resource limit specifies the maximum amount of resources that a container is allowed to use. If a container tries to exceed these limit.
When it comes to resource limits in Kubernetes, they refer to the maximum amount of a resource (such as CPU, memory, or disk space) that a container or pod can consume.

resources:
  limits:
    memory: "512Mi"
    cpu: "1"
This means the container can use a maximum of 512Mi of memory and up to 1 CPU core.
############################################################
apiVersion: v1
kind: Pod
metadata:
  name: resource-limits-example
spec:
  containers:
  - name: mycontainer
    image: myimage
    resources:
      requests:
        memory: "256Mi"
        cpu: "500m"
      limits:
        memory: "512Mi"
        cpu: "1"
TLS (Transport Layer Security) in Kubernetes enables secure communication between pods and services. 
Kubernetes uses TLS certificates and keys to establish secure connections. 
You can configure TLS in Kubernetes using ConfigMaps, Secrets, and Certificates. 
ConfigMaps and Secrets store sensitive data, such as certificates and keys, which can be used to generate a certificate signing request (CSR) or a self-signed certificate.
Certificates can be generated using tools like OpenSSL or CertManager. 
This setup ensures secure communication within your Kubernetes cluster and protects against man-in-the-middle attacks.

In Kubernetes, TLS is typically used to secure communication between different components, such as:

Between external clients and services (e.g., HTTPS traffic).
Between services within the cluster.
For securing communication with the Kubernetes API server.

1. Generate TLS Certificates
You can generate TLS certificates using openssl or another certificate authority (CA).

For example, to create a TLS certificate for a service:

# Generate a private key
openssl genrsa -out tls.key 2048

# Create a Certificate Signing Request (CSR)
openssl req -new -key tls.key -out tls.csr \
  -subj "/CN=yourdomain.com/O=yourorganization"

# Self-sign the certificate (or you can use a CA)
openssl x509 -req -in tls.csr -signkey tls.key -out tls.crt -days 365
This creates:

tls.key: Private key for your service.
tls.crt: TLS certificate (signed or self-signed).


2. Create a Kubernetes Secret for TLS
Once you have the TLS key and certificate, you can create a Kubernetes Secret to store them. 
This secret can then be referenced by services or Ingress resources for HTTPS encryption.

kubectl create secret tls tls-secret --cert=tls.crt --key=tls.key

This command creates a secret named tls-secret containing the TLS certificate and key.


3.. Use TLS in Ingress
To expose a service using HTTPS via an Ingress, you can configure the Ingress resource to reference the TLS secret.

      apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"  # Redirect HTTP to HTTPS
spec:
  tls:
  - hosts:
      - example.com
    secretName: tls-secret  # The secret that contains the TLS certificate and private key
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-service  # Name of the backend service to route traffic to
            port:
              number: 80      # The port on which the service is running

metadata.name: This is the name of the Ingress resource (example-ingress).

annotations: The nginx.ingress.kubernetes.io/ssl-redirect: "true" annotation ensures that HTTP requests are automatically redirected to HTTPS.

spec.tls:
hosts: Defines the domain (example.com) that this Ingress should secure using TLS.

secretName: The name of the Kubernetes Secret (tls-secret) that contains the TLS certificate (tls.crt) and private key (tls.key). This secret should have been created beforehand with the command:

kubectl create secret tls tls-secret --cert=tls.crt --key=tls.key

spec.rules:
host: Defines the host name (example.com) for which this Ingress rule applies.

http.paths:
path: The path to match in HTTP requests (in this case, / matches all paths).

backend: Defines the backend service to which traffic should be forwarded. It specifies the service name (my-service) and the port number (80) where the service is listening.
Usage:
This Ingress resource will:

Secure traffic to example.com using the TLS certificate stored in the tls-secret.
Redirect all HTTP traffic to HTTPS.
Route HTTPS requests for example.com to the my-service service on port 80.
Make sure that:

The TLS secret (tls-secret) contains a valid certificate and private key.
An Ingress controller, such as NGINX Ingress Controller, is deployed in your Kubernetes cluster to handle the Ingress resource.


4. Secure Internal Communication
You can also use TLS to secure communication between services inside the Kubernetes cluster. 
This involves ensuring services support TLS and configuring Secrets for them to use the necessary certificates and keys.

For example, a pod could mount the TLS certificates from a Kubernetes Secret as a volume and configure the service to use those certificates for encrypted communication.

5. Verify HTTPS
Once the Ingress is configured and deployed, you can access your service via HTTPS. 
The browser or client should trust the certificate if it's issued by a trusted Certificate Authority (CA),
or you may need to add the self-signed certificate to the trusted store


Use Cases for TLS Certificates in Kubernetes
API Server Authentication:
The Kubernetes API server uses TLS certificates for securing communications with clients (e.g., kubectl) and nodes (e.g., kubelet).
Service-to-Service Communication:
Pods and services within the cluster can use mutual TLS (mTLS) to authenticate and encrypt communication between each other.
Ingress Controllers:
TLS certificates are used to secure external access to services exposed via Ingress controllers.
etcd Encryption:
etcd, the Kubernetes cluster state store, uses TLS for secure communication between nodes and the API server.
Kubelet and API Server Communication:
TLS certificates authenticate and encrypt communication between the kubelet and the Kubernetes API server.

###########################################################################################################

Kubeconfig:

Kubeconfig is a configuration file used by kubectl and other Kubernetes clients to interact with Kubernetes clusters.
It contains information about cluster connections, authentication credentials, namespaces, and other configuration details necessary to manage a Kubernetes environment.
In simple terms, kubeconfig is the file that holds the context and connection details for interacting with different Kubernetes clusters. It allows users to manage multiple clusters and switch between them as needed.

We can move these information to a configuration file called kubeconfig. And the specify this file as the kubeconfig option in the command.

kubectl get pods --kubeconfig config

To view the current file being used

$ kubectl config view

Set or modify the kubeconfig file:
You can specify a custom kubeconfig file using the --kubeconfig flag:
kubectl --kubeconfig=/path/to/your/kubeconfig get pods

#########################################################################################################
Cluster Roles:

Roles and Rolebindings are namespaced meaning they are created within namespaces.
Namespaces
Can you group or isolate nodes within a namespace?

No, those are cluster wide or cluster scoped resources. They cannot be associated to any particular namespace.
So the resources are categorized as either namespaced or cluster scoped.

To see namespaced resources

 kubectl api-resources --namespaced=true
 To see non-namespaced resources
 kubectl api-resources --namespaced=false  ---> Clusterscoped

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["nodes"]
  verbs: ["get", "list", "delete", "create"]

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator
  apiGroup: rbac.authorization.k8s.io
In Kubernetes, a Service Account is a type of account specifically used by applications or workloads running within the cluster, 
rather than by human users. Service accounts allow pods  and other Kubernetes resources to securely interact with the Kubernetes API, and 
perform actions such as creating persistent volumes, deploying applications, and interacting with other resources.
They help enforce security by restricting what the application can access within the cluster.
Each namespace has a default service account that is automatically attached to pods if no other service account is specified.

A Service Account in Kubernetes is an identity used by processes running inside a pod to interact with the Kubernetes API server.
By default, Kubernetes assigns the default Service Account to all pods unless you specify otherwise. 
You can create and assign custom Service Accounts to pods to grant specific permissions based on RBAC policies.

Using Service Acount in a Pod:

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  namespace: default
spec:
  serviceAccountName: custom-service-account
  containers:
  - name: my-container
    image: my-application-image

################################

Granting permission to a service acount

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]


apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: custom-service-account
  namespace: default
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io





There are 5 types of services:
ClusterIP
NodePort
LoadBalancer
External Name
Headless

ClusterIP:

ClusterIP is the default service type in Kubernetes. 
It exposes the service on an internal IP address within the Kubernetes cluster, making it accessible only from within the cluster.
This is useful for internal services that do not need to be exposed to external traffic.

How ClusterIP Works
When you create a ClusterIP service, Kubernetes allocates an IP address for the service within the cluster. 
This IP is only reachable from within the cluster, and the service routes traffic to the pods that match the service's selector.

Pod yaml file:

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80

############################################

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: nginx  # Label selector to find the pods this service will expose
  ports:
    - protocol: TCP
      port: 80        # Port to access the service
      targetPort: 8080  # Port on the pod that the service will forward traffic to
  type: ClusterIP  # The type of service (defaults to ClusterIP)

####################################################
NodePort:

In Kubernetes, a NodePort is a type of service that exposes a container's port on each machine (node) in a cluster. 
It allows external traffic to access the service directly by using a static port number on each node. 
NodePorts are typically used for development environments, testing, and demos. When a NodePort is created,
Kubernetes allocates a unique port number on each node's exposed ports (30000-32767) and maps it to the container's port.
This allows clients to access the service using the node's IP and the allocated port number.

A NodePort is a type of service in Kubernetes that exposes your application on a specific port on each node in your cluster. 
This allows external traffic to access your application by using the IP address of any node in the cluster along with the specified port.

apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
      nodePort: 30007

#####################################################
Load Balancer:

A LoadBalancer service in Kubernetes is used to expose a service to external traffic by provisioning a load balancer provided by the underlying cloud infrastructure (like AWS, Google Cloud, Azure, etc.).
This type of service automatically distributes incoming traffic across multiple pods and ensures high availability and fault tolerance for your application.

How LoadBalancer Works
When you create a LoadBalancer service, Kubernetes requests a cloud provider to provision an external load balancer. 
The load balancer receives incoming traffic and forwards it to the backend pods associated with the service.

External Load Balancer:

Typically provisioned by a cloud provider.
Distributes traffic from external clients (e.g., the internet) to services running within the Kubernetes cluster.
Example: AWS Elastic Load Balancer (ELB), Google Cloud Load Balancer, Azure Load Balancer.

Internal Load Balancer:

Distributes traffic within a private network or between services inside a cluster.
Often used for communication between microservices.

It automatically provisions an external load balancer when you create a Service of type LoadBalancer.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-api-deployment
  labels:
    app: my-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-api
  template:
    metadata:
      labels:
        app: my-api
    spec:
      containers:
      - name: my-api-container
        image: my-api-image:latest
        ports:
        - containerPort: 80
      

apiVersion: v1
kind: Service
metadata:
  name: my-api-loadbalancer-service
spec:
  selector:
    app: my-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer


An external load balancer is created.
It forwards traffic from port 80 of the load balancer to port 8080 on the Pods matching the selector app: my-app.

#######################################################################
External Name:

Description: Maps a Service to an external DNS name.
Use Case: For redirecting traffic to services outside the Kubernetes cluster.
Access: Resolves to the specified external DNS name.
Example:
yaml
Copy code
apiVersion: v1
kind: Service
metadata:
  name: my-externalname-service
spec:
  type: ExternalName
  externalName: example.com

Does not define a Pod selector.
Useful for integrating with external databases or third-party services.

#####################################################################
Headless Service:

A Headless Service in Kubernetes is a specialized type of Service that does not assign a ClusterIP. 
Instead, it provides direct access to the individual Pods associated with it. 
This is typically used for stateful applications or scenarios where you need Pods to be accessed directly by their DNS names or IP addresses.

Statefulset yaml file:

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-app
spec:
  serviceName: my-headless-service
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-container
        image: my-app-image
        ports:
        - containerPort: 8080



apiVersion: v1
kind: Service
metadata:
  name: my-headless-service
  labels:
    app: my-app
spec:
  clusterIP: None
  selector:
    app: my-app
  ports:
    - name: http
      port: 80
      targetPort: 8080

For applications where each instance (Pod) must be uniquely addressable, such as databases or message brokers.
Commonly paired with StatefulSets to manage stateful applications, like databases (e.g., Cassandra, MySQL, Elasticsearch), where each Pod has a unique identity.
Unlike standard Services, Kubernetes does not use kube-proxy to route traffic for a Headless Service.
Unlike regular Services, a Headless Service does not load-balance requests across Pods.
Instead, it provides direct DNS resolution to the Pods behind it.
You specify clusterIP: None in the Service definition to create a Headless Service.
Each Pod in a Headless Service gets its own unique DNS record.
################################################################################################################
A StatefulSet is a Kubernetes resource that manages a set of replica pods with a stable network identifier. 
This allows containers to maintain a unique identity across pod restarts and, if necessary, scaling. Unlike Deployments, which provide 
rolling updates for stateless applications, StatefulSets are designed to manage stateful applications where data persists between pod restarts.
A StatefulSet ensures that each pod has a unique identity and that the order of pod creation is preserved, 
making it a crucial tool for applications that require persistent storage and consistent network connections.

A StatefulSet named web is created with 3 replicas.
Each pod in the StatefulSet will have a unique name (like web-0, web-1, web-2).
Each pod is associated with a PersistentVolume that will store data even if the pod is restarted.
it will create our pods one by one.first it will create web-0 once it is created then it will create web-1 otherwise it wont create web-1




Initialising Kubernetes... done

controlplane $ vi statefulsets.yml
controlplane $ kubectl create -f statefulsets.yml
statefulset.apps/nginx created
controlplane $ kubectl get pods
No resources found in default namespace.
controlplane $ kubectl get pods
No resources found in default namespace.
controlplane $ kubectl get all 
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   16d

NAME                     READY   AGE
statefulset.apps/nginx   0/3     87s
controlplane $ kubectl get statefulset.apps/nginx
NAME    READY   AGE
nginx   0/3     2m12s
controlplane $ kubectl get statefulset.apps/nginx
NAME    READY   AGE
nginx   0/3     2m20s
controlplane $ kubectl describe statefulset.apps/nginx
Name:               nginx
Namespace:          default
CreationTimestamp:  Sat, 20 Jul 2024 09:17:36 +0000
Selector:           app=nginx
Labels:             <none>
Annotations:        <none>
Replicas:           3 desired | 0 total
Update Strategy:    RollingUpdate
  Partition:        0
Pods Status:        0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:         nginx:1.16
    Port:          80/TCP
    Host Port:     0/TCP
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Volume Claims:
  Name:          www
  StorageClass:  
  Labels:        <none>
  Annotations:   <none>
  Capacity:      <default>
  Access Modes:  []
Events:
  Type     Reason        Age                     From                    Message
  ----     ------        ----                    ----                    -------
  Warning  FailedCreate  4m5s (x12 over 4m15s)   statefulset-controller  create Pod nginx-0 in StatefulSet nginx failed error: failed to create PVC www-nginx-0: PersistentVolumeClaim "www-nginx-0" is invalid: [spec.accessModes: Required value: at least 1 access mode is required, spec.resources[storage]: Required value]
  Warning  FailedCreate  3m55s (x13 over 4m15s)  statefulset-controller  create Claim www-nginx-0 for Pod nginx-0 in StatefulSet nginx failed error: PersistentVolumeClaim "www-nginx-0" is invalid: [spec.accessModes: Required value: at least 1 access mode is required, spec.resources[storage]: Required value]
controlplane $ vi statefulsets.yml               
controlplane $ kubectl apply -f statefulsets.yml
statefulset.apps/web created
controlplane $ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   0/1     Pending   0          7s
controlplane $ kubectl get all
NAME        READY   STATUS              RESTARTS   AGE
pod/web-0   1/1     Running             0          28s
pod/web-1   0/1     ContainerCreating   0          13s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   16d

NAME                     READY   AGE
statefulset.apps/nginx   0/3     7m46s
statefulset.apps/web     1/3     28s
controlplane $ kubectl delete pod pod/web-0
error: there is no need to specify a resource type as a separate argument when passing arguments in resource/name form (e.g. 'kubectl get resource/<resource_name>' instead of 'kubectl get resource resource/<resource_name>'
controlplane $ kubectl delete pod web-0
pod "web-0" deleted
controlplane $ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          8s
web-1   1/1     Running   0          4m13s
web-2   1/1     Running   0          3m57s
controlplane $ kubectl delete pod web-1
pod "web-1" deleted
controlplane $ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          50s
web-1   1/1     Running   0          3s
web-2   1/1     Running   0          4m39s
controlplane $ 

controlplane $ vi headless.yml
controlplane $ kubectl create -f headless.yml
service/nginx created
controlplane $ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          8m13s
web-1   1/1     Running   0          7m26s
web-2   1/1     Running   0          12m













apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi








apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx




controlplane $ kubectl get pods
NAME   READY   STATUS    RESTARTS   AGE
vol2   1/1     Running   0          2m47s
controlplane $ kubectl describe vol2
error: the server doesn't have a resource type "vol2"
controlplane $ kubectl describe pod vol2
Name:             vol2
Namespace:        default
Priority:         0
Service Account:  default
Node:             node01/172.30.2.2
Start Time:       Fri, 19 Jul 2024 16:48:23 +0000
Labels:           <none>
Annotations:      cni.projectcalico.org/containerID: 2fd8d7f80a818cead1faca5d022f2c73089c0795931c945db567952f8ea3b6b0
                  cni.projectcalico.org/podIP: 192.168.1.4/32
                  cni.projectcalico.org/podIPs: 192.168.1.4/32
Status:           Running
IP:               192.168.1.4
IPs:
  IP:  192.168.1.4
Containers:
  centos-container2:
    Container ID:  containerd://e9db271eba278d94b7b7fd5747b5b82a8bdeae68d47f87b09295ddbbc4d0b02f
    Image:         centos:7
    Image ID:      docker.io/library/centos@sha256:be65f488b7764ad3638f236b7b515b3678369a5124c47b8d32916d6487418ea4
    Port:          <none>
    Host Port:     <none>
    Command:
      sleep
      3600
    State:          Running
      Started:      Fri, 19 Jul 2024 16:50:00 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /test from test (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-llrhv (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  test:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-llrhv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  4m16s                  default-scheduler  Successfully assigned default/vol2 to node01
  Normal   Pulling    3m21s (x3 over 4m16s)  kubelet            Pulling image "centsos:7"
  Warning  Failed     3m14s (x3 over 4m8s)   kubelet            Failed to pull image "centsos:7": failed to pull and unpack image "docker.io/library/centsos:7": failed to resolve reference "docker.io/library/centsos:7": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
  Warning  Failed     3m14s (x3 over 4m8s)   kubelet            Error: ErrImagePull
  Normal   BackOff    2m48s (x4 over 4m7s)   kubelet            Back-off pulling image "centsos:7"
  Warning  Failed     2m48s (x4 over 4m7s)   kubelet            Error: ImagePullBackOff
  Normal   Pulling    2m46s                  kubelet            Pulling image "centos:7"
  Normal   Pulled     2m40s                  kubelet            Successfully pulled image "centos:7" in 6.075s (6.075s including waiting). Image size: 76101639 bytes.
  Normal   Created    2m40s                  kubelet            Created container centos-container2
  Normal   Started    2m40s                  kubelet            Started container centos-container2
controlplane $ kubectl explain pod.spec.volumes
KIND:       Pod
VERSION:    v1

FIELD: volumes <[]Volume>


DESCRIPTION:
    List of volumes that can be mounted by containers belonging to the pod. More
    info: https://kubernetes.io/docs/concepts/storage/volumes
    Volume represents a named volume in a pod that may be accessed by any
    container in the pod.
    
FIELDS:
  awsElasticBlockStore  <AWSElasticBlockStoreVolumeSource>
    awsElasticBlockStore represents an AWS Disk resource that is attached to a
    kubelet's host machine and then exposed to the pod. More info:
    https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore

  azureDisk     <AzureDiskVolumeSource>
    azureDisk represents an Azure Data Disk mount on the host and bind mount to
    the pod.

  azureFile     <AzureFileVolumeSource>
    azureFile represents an Azure File Service mount on the host and bind mount
    to the pod.

  cephfs        <CephFSVolumeSource>
    cephFS represents a Ceph FS mount on the host that shares a pod's lifetime

  cinder        <CinderVolumeSource>
    cinder represents a cinder volume attached and mounted on kubelets host
    machine. More info: https://examples.k8s.io/mysql-cinder-pd/README.md

  configMap     <ConfigMapVolumeSource>
    configMap represents a configMap that should populate this volume

  csi   <CSIVolumeSource>
    csi (Container Storage Interface) represents ephemeral storage that is
    handled by certain external CSI drivers (Beta feature).

  downwardAPI   <DownwardAPIVolumeSource>
    downwardAPI represents downward API about the pod that should populate this
    volume

  emptyDir      <EmptyDirVolumeSource>
    emptyDir represents a temporary directory that shares a pod's lifetime. More
    info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir

  ephemeral     <EphemeralVolumeSource>
    ephemeral represents a volume that is handled by a cluster storage driver.
    The volume's lifecycle is tied to the pod that defines it - it will be
    created before the pod starts, and deleted when the pod is removed.
    
    Use this if: a) the volume is only needed while the pod runs, b) features of
    normal volumes like restoring from snapshot or capacity
       tracking are needed,
    c) the storage driver is specified through a storage class, and d) the
    storage driver supports dynamic volume provisioning through
       a PersistentVolumeClaim (see EphemeralVolumeSource for more
       information on the connection between this volume type
       and PersistentVolumeClaim).
    
    Use PersistentVolumeClaim or one of the vendor-specific APIs for volumes
    that persist for longer than the lifecycle of an individual pod.
    
    Use CSI for light-weight local ephemeral volumes if the CSI driver is meant
    to be used that way - see the documentation of the driver for more
    information.
    
    A pod can use both types of ephemeral volumes and persistent volumes at the
    same time.

  fc    <FCVolumeSource>
    fc represents a Fibre Channel resource that is attached to a kubelet's host
    machine and then exposed to the pod.

  flexVolume    <FlexVolumeSource>
    flexVolume represents a generic volume resource that is provisioned/attached
    using an exec based plugin.

  flocker       <FlockerVolumeSource>
    flocker represents a Flocker volume attached to a kubelet's host machine.
    This depends on the Flocker control service being running

  gcePersistentDisk     <GCEPersistentDiskVolumeSource>
    gcePersistentDisk represents a GCE Disk resource that is attached to a
    kubelet's host machine and then exposed to the pod. More info:
    https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk

  gitRepo       <GitRepoVolumeSource>
    gitRepo represents a git repository at a particular revision. DEPRECATED:
    GitRepo is deprecated. To provision a container with a git repo, mount an
    EmptyDir into an InitContainer that clones the repo using git, then mount
    the EmptyDir into the Pod's container.

  glusterfs     <GlusterfsVolumeSource>
    glusterfs represents a Glusterfs mount on the host that shares a pod's
    lifetime. More info: https://examples.k8s.io/volumes/glusterfs/README.md

  hostPath      <HostPathVolumeSource>
    hostPath represents a pre-existing file or directory on the host machine
    that is directly exposed to the container. This is generally used for system
    agents or other privileged things that are allowed to see the host machine.
    Most containers will NOT need this. More info:
    https://kubernetes.io/docs/concepts/storage/volumes#hostpath

  iscsi <ISCSIVolumeSource>
    iscsi represents an ISCSI Disk resource that is attached to a kubelet's host
    machine and then exposed to the pod. More info:
    https://examples.k8s.io/volumes/iscsi/README.md

  name  <string> -required-
    name of the volume. Must be a DNS_LABEL and unique within the pod. More
    info:
    https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names

  nfs   <NFSVolumeSource>
    nfs represents an NFS mount on the host that shares a pod's lifetime More
    info: https://kubernetes.io/docs/concepts/storage/volumes#nfs

  persistentVolumeClaim <PersistentVolumeClaimVolumeSource>
    persistentVolumeClaimVolumeSource represents a reference to a
    PersistentVolumeClaim in the same namespace. More info:
    https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims

  photonPersistentDisk  <PhotonPersistentDiskVolumeSource>
    photonPersistentDisk represents a PhotonController persistent disk attached
    and mounted on kubelets host machine

  portworxVolume        <PortworxVolumeSource>
    portworxVolume represents a portworx volume attached and mounted on kubelets
    host machine

  projected     <ProjectedVolumeSource>
    projected items for all in one resources secrets, configmaps, and downward
    API

  quobyte       <QuobyteVolumeSource>
    quobyte represents a Quobyte mount on the host that shares a pod's lifetime

  rbd   <RBDVolumeSource>
    rbd represents a Rados Block Device mount on the host that shares a pod's
    lifetime. More info: https://examples.k8s.io/volumes/rbd/README.md

  scaleIO       <ScaleIOVolumeSource>
    scaleIO represents a ScaleIO persistent volume attached and mounted on
    Kubernetes nodes.

  secret        <SecretVolumeSource>
    secret represents a secret that should populate this volume. More info:
    https://kubernetes.io/docs/concepts/storage/volumes#secret

  storageos     <StorageOSVolumeSource>
    storageOS represents a StorageOS volume attached and mounted on Kubernetes
    nodes.

  vsphereVolume <VsphereVirtualDiskVolumeSource>
    vsphereVolume represents a vSphere volume attached and mounted on kubelets
    host machine


controlplane $ kubectl exec -it vol2 vi /test/testfile
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
controlplane $ kubectl exec -it vol2 touch  /test/testfile
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
controlplane $ kubectl exec -it vol2 touch /test/testfile
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
controlplane $ vi volume2.yml                            
controlplane $ kubectl create -f volume2.yml
error: error parsing volume2.yml: error converting YAML to JSON: yaml: line 14: did not find expected key
controlplane $ vi volume2.yml
controlplane $ kubectl create -f volume2.yml
error: error parsing volume2.yml: error converting YAML to JSON: yaml: line 14: did not find expected key
controlplane $ kubectl apply -f volume2.yml
error: error parsing volume2.yml: error converting YAML to JSON: yaml: line 14: did not find expected key
controlplane $ vi volume2.yml
controlplane $ kubectl create -f volume2.yml
error: error parsing volume2.yml: error converting YAML to JSON: yaml: line 16: mapping values are not allowed in this context
controlplane $ vi volume2.yml
controlplane $ kubectl create -f volume2.yml
pod/morevol2 created
controlplane $ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
morevol2   2/2     Running   0          6s
vol2       1/1     Running   0          33m
controlplane $ kubectl exec -it morevol2 -c centos-container1 -- touch /centos/testfile
controlplane $ kubectl exec -it morevol2 -c centos-container2 -- ls -l /centos2        
total 0
-rw-r--r-- 1 root root 0 Jul 19 17:24 testfile
controlplane $ 
By default, Kubernetes tries to schedule a pod on any available node unless the node has a taint that prevents this.
Taint:

A taint is a label applied to a node that affects the pod scheduling decision
In Kubernetes, Taints and Tolerations work together to control the scheduling of pods to nodes. 
They help ensure that specific pods are either allowed or prevented from running on certain nodes,
based on custom conditions or node-specific requirements.

"kubectl taint nodes node01 key=value:NoSchedule"  ---> This means no pod will be scheduled on node1 unless it has a toleration for this taint

 kubectl taint nodes node01 key=value:NoSchedule-
 - is used to untaint the node

Effect options:

NoSchedule: New pods that do not tolerate this taint cannot be scheduled on this node.
PreferNoSchedule: The system tries to avoid scheduling new pods that do not tolerate this taint, but it’s not guaranteed.
NoExecute: New pods and existing pods that do not tolerate this taint will be evicted or won’t be scheduled.

###############################################################################################################

Tolerations:
A toleration is applied to pods to allow them to tolerate specific taints on nodes, 
meaning they can be scheduled on nodes that have matching taints.

Tolerations allow pods to "tolerate" the taints applied to nodes.

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"



Taints are applied to nodes to control pod scheduling, either preventing pods from being scheduled or evicting them.
Tolerations are applied to pods to allow them to bypass taints and run on nodes with specific taints.

############################################################################################################################
Only pods which are tolerant to the particular taint on a node will get scheduled on that node.

kubectl taint nodes node1 app=blue:NoSchedule

To see this taint, run the below command
kubectl describe node kubemaster |grep Taint

Tolerations are added to pods by adding a tolerations section in pod definition


The pod is assigned to node1,But for example avaiables nodes have same taint,in this case we should use node selectors

kubectl label nodes node1 size=Large

apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
 containers:
 - name: nginx-container
   image: nginx
 tolerations:
 - key: "app"
   operator: "Equal"
   value: "blue"
   effect: "NoSchedule"
 nodeSelector:
  size: Large

This ensures the pod is scheduled only on nodes with the label size=Large.
myapp-pod is assigned to node1  when match the taint and label ,but here there is a issue, for example e have 2 nodes both nodes also have same taints and node selctors,i
in this case pod will assigned to available node

but we should assign a pod to specific node,this is our main goal so here we should go with Node affinity

apiVersion: v1
kind: Pod
metadata:
 name: myapp-pod
spec:
 containers:
 - name: nginx-container 
   image: nginx
 tolerations:
 - key: "app"
   operator: "Equal"
   value: "blue"
   effect: "NoSchedule"
 nodeSelector:
  size: Large
 affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions: 
        - key: node-ip
          operator: In
          values: 
          - 192.7.54.6 

Here we have specified ip address of our specific node so this pod will assigned to specific node 
if this ip address is not matched to any node the pod is on pending state

requiredDuringSchedulingIgnoredDuringExecution
This is a hard constraint: the pod must be scheduled on a node that satisfies the condition.
If no nodes meet the criteria, the pod won't be scheduled.

preferredDuringSchedulingIgnoredDuringExecution
This is a soft constraint: the scheduler will prefer nodes that satisfy the condition but will still schedule the pod on other nodes if none meet the criteria.           









controlplane $ vi deploy.yml
controlplane $ kubectl create -f deploy.yml
deployment.apps/nginx-deployment created
controlplane $ kubectl get pods
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-77d8468669-56f2v   1/1     Running   0          33s
nginx-deployment-77d8468669-tfmxs   1/1     Running   0          33s
nginx-deployment-77d8468669-w2k2z   1/1     Running   0          33s
controlplane $ kubectl exec -it nginx-deployment-77d8468669-56f2v -- /bin/bash
root@nginx-deployment-77d8468669-56f2v:/# ip a
bash: ip: command not found
root@nginx-deployment-77d8468669-56f2v:/# ps aux
bash: ps: command not found
root@nginx-deployment-77d8468669-56f2v:/# cd /proc
root@nginx-deployment-77d8468669-56f2v:/proc# ls
1          cgroups    dma          iomem      kmsg         meminfo       partitions   softirqs       timer_list         zoneinfo
14         cmdline    driver       ioports    kpagecgroup  misc          pressure     stat           tty
6          consoles   execdomains  irq        kpagecount   modules       sched_debug  swaps          uptime
7          cpuinfo    fb           kallsyms   kpageflags   mounts        schedstat    sys            version
acpi       crypto     filesystems  kcore      loadavg      mtrr          scsi         sysrq-trigger  version_signature
buddyinfo  devices    fs           key-users  locks        net           self         sysvipc        vmallocinfo
bus        diskstats  interrupts   keys       mdstat       pagetypeinfo  slabinfo     thread-self    vmstat
root@nginx-deployment-77d8468669-56f2v:/proc# cd 100
bash: cd: 100: No such file or directory
root@nginx-deployment-77d8468669-56f2v:/proc# cd 14
bash: cd: 14: No such file or directory
root@nginx-deployment-77d8468669-56f2v:/proc# cd 1
root@nginx-deployment-77d8468669-56f2v:/proc/1# cat cmdline
nginx: master process nginx -g daemon off;root@nginx-deployment-77d8468669-56f2v:/proc/1# exit
exit
controlplane $ kubectl logs nginx-deployment-77d8468669-56f2v
controlplane $ kubectl get pods                              
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-77d8468669-56f2v   1/1     Running   0          15m
nginx-deployment-77d8468669-tfmxs   1/1     Running   0          15m
nginx-deployment-77d8468669-w2k2z   1/1     Running   0          15m
controlplane $ kubectl logs nginx-deployment-77d8468669-56f2v
controlplane $ kubectl logs nginx-deployment-77d8468669-tfmxs
controlplane $ kubectl describe pod nginx-deployment-77d8468669-56f2v
Name:             nginx-deployment-77d8468669-56f2v
Namespace:        default
Priority:         0
Service Account:  default
Node:             node01/172.30.2.2
Start Time:       Sat, 20 Jul 2024 13:08:22 +0000
Labels:           app=nginx
                  pod-template-hash=77d8468669
Annotations:      cni.projectcalico.org/containerID: 67ee01d6972a863ed6ac048eeb90a8ca8544f378c3e6738207bbb5527ca94f51
                  cni.projectcalico.org/podIP: 192.168.1.5/32
                  cni.projectcalico.org/podIPs: 192.168.1.5/32
Status:           Running
IP:               192.168.1.5
IPs:
  IP:           192.168.1.5
Controlled By:  ReplicaSet/nginx-deployment-77d8468669
Containers:
  nginx:
    Container ID:   containerd://25cf71da6f2deae867dcfccad61eb18c80f3d7faa8e5cb5c7d168c50508ce8aa
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sat, 20 Jul 2024 13:08:28 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cfn6q (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-cfn6q:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  17m   default-scheduler  Successfully assigned default/nginx-deployment-77d8468669-56f2v to node01
  Normal  Pulling    17m   kubelet            Pulling image "nginx:1.14.2"
  Normal  Pulled     17m   kubelet            Successfully pulled image "nginx:1.14.2" in 4.808s (4.808s including waiting). Image size: 44710204 bytes.
  Normal  Created    17m   kubelet            Created container nginx
  Normal  Started    17m   kubelet            Started container nginx
controlplane $ kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   16d   v1.30.0
node01         Ready    <none>          16d   v1.30.0
controlplane $ kubectl cordon -h
Mark node as unschedulable.

Examples:
  # Mark node "foo" as unschedulable
  kubectl cordon foo

Options:
    --dry-run='none':
        Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without
        sending it. If server strategy, submit server-side request without persisting the resource.

    -l, --selector='':
        Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching
        objects must satisfy all of the specified label constraints.

Usage:
  kubectl cordon NODE [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
controlplane $ kubectl drain -h
Drain node in preparation for maintenance.

 The given node will be marked unschedulable to prevent new pods from arriving. 'drain' evicts the pods if the API
server supports https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ eviction
https://kubernetes.io/docs/concepts/workloads/pods/disruptions/ . Otherwise, it will use normal DELETE to delete the
pods. The 'drain' evicts or deletes all pods except mirror pods (which cannot be deleted through the API server).  If
there are daemon set-managed pods, drain will not proceed without --ignore-daemonsets, and regardless it will not delete
any daemon set-managed pods, because those pods would be immediately replaced by the daemon set controller, which
ignores unschedulable markings.  If there are any pods that are neither mirror pods nor managed by a replication
controller, replica set, daemon set, stateful set, or job, then drain will not delete any pods unless you use --force.
--force will also allow deletion to proceed if the managing resource of one or more pods is missing.

 'drain' waits for graceful termination. You should not operate on the machine until the command completes.

 When you are ready to put the node back into service, use kubectl uncordon, which will make the node schedulable again.

https://kubernetes.io/images/docs/kubectl_drain.svg Workflowhttps://kubernetes.io/images/docs/kubectl_drain.svg

Examples:
  # Drain node "foo", even if there are pods not managed by a replication controller, replica set, job, daemon set, or
stateful set on it
  kubectl drain foo --force
  
  # As above, but abort if there are pods not managed by a replication controller, replica set, job, daemon set, or
stateful set, and use a grace period of 15 minutes
  kubectl drain foo --grace-period=900

Options:
    --chunk-size=500:
        Return large lists in chunks rather than all at once. Pass 0 to disable. This flag is beta and may change in
        the future.

    --delete-emptydir-data=false:
        Continue even if there are pods using emptyDir (local data that will be deleted when the node is drained).

    --disable-eviction=false:
        Force drain to use delete, even if eviction is supported. This will bypass checking PodDisruptionBudgets, use
        with caution.

    --dry-run='none':
        Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without
        sending it. If server strategy, submit server-side request without persisting the resource.

    --force=false:
        Continue even if there are pods that do not declare a controller.

    --grace-period=-1:
        Period of time in seconds given to each pod to terminate gracefully. If negative, the default value specified
        in the pod will be used.

    --ignore-daemonsets=false:
        Ignore DaemonSet-managed pods.

    --pod-selector='':
        Label selector to filter pods on the node

    -l, --selector='':
        Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching
        objects must satisfy all of the specified label constraints.

    --skip-wait-for-delete-timeout=0:
        If pod DeletionTimestamp older than N seconds, skip waiting for the pod.  Seconds must be greater than 0 to
        skip.

    --timeout=0s:
        The length of time to wait before giving up, zero means infinite

Usage:
  kubectl drain NODE [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
controlplane $ kubectl uncordon -h
Mark node as schedulable.

Examples:
  # Mark node "foo" as schedulable
  kubectl uncordon foo

Options:
    --dry-run='none':
        Must be "none", "server", or "client". If client strategy, only print the object that would be sent, without
        sending it. If server strategy, submit server-side request without persisting the resource.

    -l, --selector='':
        Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching
        objects must satisfy all of the specified label constraints.

Usage:
  kubectl uncordon NODE [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).









Deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
